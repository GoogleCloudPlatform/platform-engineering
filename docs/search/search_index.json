{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Platform Engineering on Google Cloud","text":"<p>Platform engineering is an emerging practice in organizations to enable cross functional collaboration in order to deliver business value faster. It treats the internal groups; application developers, operators, security, infrastructure admins, etc. as customers and provides them the foundational platforms to accelerate their work. The key goals of platform engineering are providing everything as self-service, golden paths, improved collaboration, abstraction of technical complexities, all of which simplify the software development lifecycle, contributing towards delivering business values to consumers. Platform engineering is more effective in cloud computing as it helps realize the benefits possible on cloud like automation, security, productivity, faster time-to-market.</p>"},{"location":"#overview","title":"Overview","text":"<p>Google Cloud offers decomposable, elastic, secure, scalable and cost efficient tools built on the guiding principles of platform engineering. With a focus on developer experience and innovation coupled with practices like SRE embedded into the tools, they make a good place to begin your platform journey to empower the developers to enhance their experience and increase their productivity.</p> <p>This repository contains a collection of guides, examples and design patterns spanning Google Cloud products and best in class OSS tools, which you can use to help build an internal developer platform.</p> <p>For more information, see Platform Engineering on Google Cloud.</p>"},{"location":"#resources","title":"Resources","text":""},{"location":"#design-patterns","title":"Design Patterns","text":"<ul> <li>Platform Engineering: 5 Implemenation Myths</li> <li>Business continuity planning for CI/CD</li> </ul>"},{"location":"#research-papers-and-white-papers","title":"Research papers and white papers","text":"<ul> <li>Google Cloud ESG Strategic Guide: Discover the power of platform   engineering</li> <li>Mastering Platform Engineering: Key Insights from Industry   Experts</li> </ul>"},{"location":"#guides-and-building-blocks","title":"Guides and Building Blocks","text":""},{"location":"#manage-developer-environments-at-scale","title":"Manage Developer Environments at Scale","text":"<ul> <li>Backstage Plugin for Cloud Workstations</li> </ul>"},{"location":"#self-service-and-automation-patterns","title":"Self-service and Automation patterns","text":"<ul> <li>Automatic password rotation</li> </ul>"},{"location":"#run-third-party-cicd-tools-on-google-cloud-infrastructure","title":"Run third-party CI/CD tools on Google Cloud infrastructure","text":"<ul> <li>Host GitHub Actions Runners on GKE</li> </ul>"},{"location":"#enterprise-change-management","title":"Enterprise change management","text":"<ul> <li>Integrate Cloud Deploy with enterprise change management   systems</li> </ul>"},{"location":"#end-to-end-examples","title":"End-to-end Examples","text":"<ul> <li>Enterprise Application Blueprint - Deploys an   internal developer platform that enables cloud platform teams to provide a   managed software development and delivery platform for their organization's   application development groups. EAB builds upon the infrastructure foundation   deployed using the Enterprise Foundation   blueprint.</li> <li>Software Delivery Blueprint - An opinionated   approach using platform engineering to improve software delivery, specifically   for Infrastructure admins, Operators, Security specialists, and Application   developers. It utilizes GitOps and self-service workflows to enable consistent   infrastructure, automated security policies, and autonomous application   deployment for developers.</li> </ul>"},{"location":"#usage-disclaimer","title":"Usage Disclaimer","text":"<p>Copy any code you need from this repository into your own project.</p> <p>Warning: Do not depend directly on the samples in this repository. Breaking changes may be made at any time without warning.</p>"},{"location":"#contributing-changes","title":"Contributing changes","text":"<p>Entirely new samples are not accepted. Bugfixes are welcome, either as pull requests or as GitHub issues.</p> <p>See CONTRIBUTING.md for details on how to contribute.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>Copyright 2024 Google LLC Code in this repository is licensed under the Apache 2.0. See LICENSE.</p>"},{"location":"code-of-conduct/","title":"Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project email address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p> <p>This Code of Conduct also applies outside the project spaces when the Project Steward has a reasonable belief that an individual's behavior may have a negative impact on the project or its community.</p>"},{"location":"code-of-conduct/#conflict-resolution","title":"Conflict Resolution","text":"<p>We do not believe that all conflict is bad; healthy debate and disagreement often yield positive results. However, it is never okay to be disrespectful or to engage in behavior that violates the project\u2019s code of conduct.</p> <p>If you see someone violating the code of conduct, you are encouraged to address the behavior directly with those involved. Many issues can be resolved quickly and easily, and this gives people more control over the outcome of their dispute. If you are unable to resolve the matter for any reason, or if the behavior is threatening or harassing, report it. We are dedicated to providing an environment where participants feel welcome and safe.</p> <p>Reports should be directed to [PROJECT STEWARD NAME(s) AND EMAIL(s)], the Project Steward(s) for [PROJECT NAME]. It is the Project Steward\u2019s duty to receive and address reported violations of the code of conduct. They will then work with a committee consisting of representatives from the Open Source Programs Office and the Google Open Source Strategy team. If for any reason you are uncomfortable reaching out to the Project Steward, please email opensource@google.com.</p> <p>We will investigate every complaint, but you may not receive a direct response. We will use our discretion in determining when and how to follow up on reported incidents, which may range from not taking action to permanent expulsion from the project and project-sponsored spaces. We will notify the accused of the report and provide them an opportunity to discuss it before any action is taken. The identity of the reporter will be omitted from the details of the report supplied to the accused. In potentially harmful situations, such as ongoing harassment or threats to anyone's safety, we may take action without notice.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"contributing/","title":"How to Contribute","text":"<p>We'd love to accept your patches and contributions to this project.</p>"},{"location":"contributing/#before-you-begin","title":"Before you begin","text":""},{"location":"contributing/#sign-our-contributor-license-agreement","title":"Sign our Contributor License Agreement","text":"<p>Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project.</p> <p>If you or your current employer have already signed the Google CLA (even if it was for a different project), you probably don't need to do it again.</p> <p>Visit https://cla.developers.google.com/ to see your current agreements or to sign a new one.</p>"},{"location":"contributing/#review-our-community-guidelines","title":"Review our Community Guidelines","text":"<p>This project follows Google's Open Source Community Guidelines.</p>"},{"location":"contributing/#contribution-process","title":"Contribution process","text":""},{"location":"contributing/#code-reviews","title":"Code Reviews","text":"<p>All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.</p>"},{"location":"contributing/#development-guide","title":"Development guide","text":"<p>This document contains technical information to contribute to this repository.</p>"},{"location":"contributing/#site","title":"Site","text":"<p>This repository includes scripts and configuration to build a site using Material for MkDocs:</p> <ul> <li><code>config/mkdocs</code>: MkDocs configuration files</li> <li><code>scripts/run-mkdocs.sh</code>: script to build the site</li> <li><code>.github/workflows/documentation.yaml</code>: GitHub Actions workflow that builds   the site, and pushes a commit with changes on the current branch.</li> </ul>"},{"location":"contributing/#build-the-site","title":"Build the site","text":"<p>To build the site, run the following command from the root of the repository:</p> <pre><code>scripts/run-mkdocs.sh\n</code></pre>"},{"location":"contributing/#preview-the-site","title":"Preview the site","text":"<p>To preview the site, run the following command from the root of the repository:</p> <pre><code>scripts/run-mkdocs.sh \"serve\"\n</code></pre>"},{"location":"contributing/#linting-and-formatting","title":"Linting and formatting","text":"<p>We configured several linters and formatters for code and documentation in this repository. Linting and formatting checks run as part of CI workflows.</p> <p>Linting and formatting checks are configured to check changed files only by default. If you change the configuration of any linter or formatter, these checks run against the entire repository.</p> <p>To run linting and formatting checks locally, you do the following:</p> <pre><code>scripts/lint.sh\n</code></pre> <p>To automatically fix certain linting and formatting errors, you do the following:</p> <pre><code>LINTER_CONTAINER_FIX_MODE=\"true\" scripts/lint.sh\n</code></pre>"},{"location":"reference-architectures/automated-password-rotation/","title":"Overview","text":"<p>Secrets rotation is a broadly accepted best practice across the information technology industry. However, often times it is cumbersome and disruptive process. In this guide you will use Google Cloud tools to automate the process of rotating passwords for a Cloud SQL instance. This method could easily be extended to other tools and types of secrets.</p>"},{"location":"reference-architectures/automated-password-rotation/#storing-passwords-in-google-cloud","title":"Storing passwords in Google Cloud","text":"<p>In Google Cloud, secrets including passwords can be stored using many different tools including common open source tools such as Vault, however in this guide, you will use Secret Manager, Google Cloud's fully managed product for securely storing secrets. Regardless of the tool you use, passwords stored should be further secured. When using Secret Manager, following are some of the ways you can further secure your secrets:</p> <ol> <li> <p>Limiting access : The secrets should be readable writable only through     the Service Accounts via IAM roles. The principle     of least privilege must be followed while granting roles to the service     accounts.</p> </li> <li> <p>Encryption : The secrets should be encrypted. Secret     Manager encrypts the secret at rest using AES-256 by     default. But you can use your own encryption keys, customer-managed     encryption keys (CMEK) to encrypt your secret at rest. For details, see     Enable customer-managed encryption keys for Secret     Manager.</p> </li> <li> <p>Password rotation : The passwords stored in the secret manager should be     rotated on a regular basis to reduce the risk of a security incident.</p> </li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#why-password-rotation","title":"Why password rotation","text":"<p>Security best practices require us to regularly rotate the passwords in our stack. Changing the password mitigates the risk in the event where passwords are compromised.</p>"},{"location":"reference-architectures/automated-password-rotation/#how-to-rotate-passwords","title":"How to rotate passwords","text":"<p>Manually rotating the passwords is an antipattern and should not be done as it exposes the password to the human rotating it and may result in security and system incidents. Manual rotation processes also introduce the risk that the rotation isn't actually performed due to human error, for example forgetting or typos.</p> <p>This necessitates having a workflow that automates password rotation. The password could be of an application, a database, a third-party service or a SaaS vendor etc.</p>"},{"location":"reference-architectures/automated-password-rotation/#automatic-password-rotation","title":"Automatic password rotation","text":"<p>Typically, rotating a password requires these steps:</p> <ul> <li>Change the password in the underlying software or system</li> </ul> <p>(such as applications,databases, SaaS).</p> <ul> <li> <p>Update Secret Manager to store the new password.</p> </li> <li> <p>Restart the applications that use that password. This will make the</p> </li> </ul> <p>application source the latest passwords.</p> <p>The following architecture represents a general design for a systems that can rotate password for any underlying software/system.</p> <p></p>"},{"location":"reference-architectures/automated-password-rotation/#workflow","title":"Workflow","text":"<ul> <li>A pipeline or a cloud scheduler job sends a message to a   pub/sub topic. The message contains the information about the password that is   to be rotated. For example, this information may include secret ID in secret   manager, database instance and username if it is a database password.</li> <li>The message arriving to the pub/sub topic triggers a Cloud Run   Function that reads the message and gathers information as   supplied in the message.</li> <li>The function changes the password in the corresponding system. For example, if   the message contained a database instance, database name and user,the function   changes the password for that user in the given database.</li> <li>The function updates the password in secret manager to reflect the new   password. It knows what secret ID to update since it was provided in the   pub/sub message.</li> <li>The function publishes a message to a different pub/sub topic indicating that   the password has been rotated. This topic can be subscribed any application or   system that may want to know in the event of password rotation, whether to   re-start themselves or perform any other task.</li> </ul>"},{"location":"reference-architectures/automated-password-rotation/#example-deployment-for-automatic-password-rotation-in-cloudsql","title":"Example deployment for automatic password rotation in CloudSQL","text":"<p>The following architecture demonstrates a way to automatically rotate CloudSQL password.</p> <p></p>"},{"location":"reference-architectures/automated-password-rotation/#workflow-of-the-example-deployment","title":"Workflow of the example deployment","text":"<ul> <li>A Cloud Scheduler job is scheduled to run every 1st day on   the month. The jobs publishes a message to a Pub/Sub topic containing secret   ID, Cloud SQL instance name, database, region and database user in the   payload.</li> <li>The message arrival on the pub/sub topic triggers a Cloud Run   Function, which uses the information provided in the message   to connect to the CloudSQL instance via Serverless VPC   Connector and changes the password. The function uses a   service account that has IAM roles required to   connect to the Cloud Sql instance.</li> <li>The function then updates the secret in Secret Manager.</li> </ul> <p>Note : The architecture doesn't show the flow to restart the application after the password rotation as shown in thee Generic architecture but it can be added easily with minimal changes to the Terraform code.</p>"},{"location":"reference-architectures/automated-password-rotation/#deploy-the-architecture","title":"Deploy the architecture","text":"<p>The code to build the architecture has been provided with this repository. Follow these instructions to create the architecture and use it:</p> <ol> <li> <p>Open Cloud Shell on Google Cloud Console and log in with your     credentials.</p> </li> <li> <p>If you want to use an existing project, get <code>role/project.owner</code> role on the     project and set the environment in Cloud Shell as shown below. Then, move to     step 4.</p> <pre><code> #set shell environment variable\n export PROJECT_ID=&lt;PROJECT_ID&gt;\n</code></pre> <p>Replace <code>&lt;PROJECT_ID&gt;</code> with the ID of the existing project.</p> </li> <li> <p>If you want to create a new GCP project run the following commands in Cloud     Shell.</p> <pre><code> #set shell environment variable\n export PROJECT_ID=&lt;PROJECT_ID&gt;\n #create project\n gcloud projects create ${PROJECT_ID} --folder=&lt;FOLDER_ID&gt;\n #associate the project with billing account\n gcloud billing projects link ${PROJECT_ID} --billing-account=&lt;BILLING_ACCOUNT_ID&gt;\n</code></pre> <p>Replace <code>&lt;PROJECT_ID&gt;</code> with the ID of the new project. Replace <code>&lt;BILLING_ACCOUNT_ID&gt;</code> with the billing account ID that the project should be associated with.</p> </li> <li> <p>Set the project ID in Cloud Shell and enable APIs in the project:</p> <pre><code> gcloud config set project ${PROJECT_ID}\n gcloud services enable \\\n  cloudresourcemanager.googleapis.com \\\n  serviceusage.googleapis.com \\\n  --project ${PROJECT_ID}\n</code></pre> </li> <li> <p>Download the Git repository containing the code to build the example     architecture:</p> <pre><code> cd ~\n git clone https://github.com/GoogleCloudPlatform/platform-engineering\n cd platform-engineering/reference-architectures/automated-password-rotation/terraform\n\n terraform init\n terraform plan -var \"project_id=$PROJECT_ID\"\n terraform apply -var \"project_id=$PROJECT_ID\" --auto-approve\n</code></pre> <p>Note: It takes around 30 mins for the entire architecture to get deployed.</p> </li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#review-the-deployed-architecture","title":"Review the deployed architecture","text":"<p>Once the Terraform apply has successfully finished, the example architecture will be deployed in the your Google Cloud project. Before exercising the rotation process, review and verify the deployment in the Google Cloud Console.</p>"},{"location":"reference-architectures/automated-password-rotation/#review-cloud-sql-database","title":"Review Cloud SQL database","text":"<ol> <li>In the Cloud Console, using the naviagion menu select <code>Databases &gt; SQL</code>.     Confirm that <code>cloudsql-for-pg</code> is present in the instance list.</li> <li>Click on <code>cloudsql-for-pg</code>, to open the instance details page.</li> <li>In the left hand menu select <code>Users</code>. Confirm you see a user with the name     <code>user1</code>.</li> <li>In the left hand menu select <code>Databases</code>. Confirm you see see a database     named <code>test</code>.</li> <li>In the left hand menu select <code>Overview</code>.</li> <li>In the <code>Connect to this instance</code> section, note that only     <code>Private IP address</code> is present and no public IP address. This restricts     access to the instance over public network.</li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#review-secret-manager","title":"Review Secret Manager","text":"<ol> <li>In the Cloud Console, using the naviagion menu select     <code>Security &gt; Secret Manager</code>. Confirm that <code>cloudsql-pswd</code> is present in the     list.</li> <li>Click on <code>cloudsql-pswd</code>.</li> <li>Click three dots icon and select <code>View secret value</code> to view the password     for Cloud SQL database.</li> <li>Copy the secret value, you will use this in the next section to confirm     access to the Cloud SQL instance.</li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#review-cloud-scheduler-job","title":"Review Cloud Scheduler job","text":"<ol> <li>In the Cloud Console, using the naviagion menu select     <code>Integration Services &gt; Cloud Scheduler</code>. Confirm that     <code>password-rotator-job</code> is present in the Scheduler Jobs list.</li> <li>Click on <code>password-rotator-job</code>, confirm it is configured to run on 1st of     every month.</li> <li> <p>Click <code>Continue</code> to see execution configuration. Confirm the following     settings:</p> <ul> <li><code>Target type</code> is Pub/Sub</li> <li><code>Select a Cloud Pub/Sub topic</code> is set to <code>pswd-rotation-topic</code></li> <li><code>Message body</code> contains a JSON object with the details of the Cloud SQL   isntance and secret to be rotated.</li> </ul> </li> <li> <p>Click <code>Cancel</code>, to exit the Cloud Scheduler job details.</p> </li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#review-pubsub-topic-configuration","title":"Review Pub/Sub topic configuration","text":"<ol> <li>In the Cloud Console, using the naviagion menu select <code>Analytics &gt; Pub/Sub</code>.</li> <li>In the left hand menu select <code>Topic</code>. Confirm that <code>pswd-rotation-topic</code> is     present in the topics list.</li> <li>Click on <code>pswd-rotation-topic</code>.</li> <li>In the <code>Subscriptions</code> tab, click on Subscription ID for the rotator Cloud     Function.</li> <li>Click on the <code>Details</code> tab. Confirm, the <code>Audience</code> tag shows the rotator     Cloud Function.</li> <li>In the left hand menu select <code>Topic</code>.</li> <li>Click on <code>pswd-rotation-topic</code>.</li> <li>Click on the <code>Details</code> tab.</li> <li>Click on the schema in the <code>Schema name</code> field.</li> <li>In the <code>Details</code>, confirm that the schema contains these keys: <code>secretid</code>,     <code>instance_name</code>, <code>db_user</code>, <code>db_name</code> and <code>db_location</code>. These keys will be     used to identify what database and user password is to be rotated.</li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#review-cloud-run-function","title":"Review Cloud Run Function","text":"<ol> <li>In the Cloud Console, using the naviagion menu select     <code>Serverless &gt; Cloud Run Functions</code>. Confirm that <code>pswd_rotator_function</code> is     present in the list.</li> <li>Click on <code>pswd_rotator_function</code>.</li> <li>Click on the <code>Trigger</code> tab. Confirm that the field <code>Receive events from</code> has     the Pub/Sub topic <code>pswd-rotation-topic</code>. This indicates that the function     will run when a message arrives to that topic.</li> <li>Click on the <code>Details</code> tab. Confirm that under <code>Network Settings</code> VPC     connector is set to <code>connector-for-sql</code>. This allows the function to connect     to the CloudSQL over private IPs.</li> <li>Click on the <code>Source</code> tab to see the python code that the function executes.</li> </ol> <p>Note: For the purpose of this tutorial, the secret is accessible to the human users and not encrypted. See the section and Secret Manager best practice</p>"},{"location":"reference-architectures/automated-password-rotation/#verify-that-you-are-able-to-connect-to-the-cloud-sql-instance","title":"Verify that you are able to connect to the Cloud SQL instance","text":"<ol> <li>In the Cloud Console, using the naviagion menu select <code>Databases &gt; SQL</code></li> <li>Click on <code>cloudsql-for-pg</code></li> <li>In the left hand menu select <code>Cloud SQL Studio</code>.</li> <li>In <code>Database</code> dropdown, choose <code>test</code>.</li> <li>In <code>User</code> dropdown, choose <code>user1</code>.</li> <li>In <code>Password</code> textbox paste the password copied from the <code>cloudsql-pswd</code>     secret.</li> <li>Click <code>Authenticate</code>. Confirm you were able to log in to the database.</li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#rotate-the-cloud-sql-password","title":"Rotate the Cloud SQL password","text":"<p>Typically, the Cloud Scheduler will automatically run on 1st day of every month triggering password rotation. However, for this tutorial you will run the Cloud Scheduler job manually, which causes the Cloud Run Function to generate a new password, update it in Cloud SQL and store it in Secret Manager.</p> <ol> <li>In the Cloud Console, using the naviagion menu select     <code>Integration Services &gt; Cloud Scheduler</code>.</li> <li>For the scheduler job <code>password-rotator-job</code>. Click the three dots icon and     select <code>Force run</code>.</li> <li>Verify that the <code>Status of last execution</code> shows <code>Success</code>.</li> <li>In the Cloud Console, using the naviagion menu select     <code>Serverless &gt; Cloud Run Functions</code>.</li> <li>Click function named <code>pswd_rotator_function</code>.</li> <li>Select the <code>Logs</code> tab.</li> <li>Review the logs and verify the function has run and completed without     errors. Successful completion will be noted with log entries containing     <code>Secret cloudsql-pswd changed in Secret Manager!</code>,     <code>DB password changed successfully!</code> and     <code>DB password verified successfully!</code>.</li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#test-the-new-password","title":"Test the new password","text":"<ol> <li>In the Cloud Console, using the naviagion menu select     <code>Security &gt; Secret Manager</code>. Confirm that <code>cloudsql-pswd</code> is present in the     list.</li> <li>Click on <code>cloudsql-pswd</code>. Note you should now see a new version, version 2     of the secret.</li> <li>Click three dots icon and select <code>View secret value</code> to view the password     for Cloud SQL database.</li> <li>Copy the secret value.</li> <li>In the Cloud Console, using the naviagion menu select <code>Databases &gt; SQL</code></li> <li>Click on <code>cloudsql-for-pg</code></li> <li>In the left hand menu select <code>Cloud SQL Studio</code>.</li> <li>In <code>Database</code> dropdown, choose <code>test</code>.</li> <li>In <code>User</code> dropdown, choose <code>user1</code>.</li> <li>In <code>Password</code> textbox paste the password copied from the <code>cloudsql-pswd</code>     secret.</li> <li>Click <code>Authenticate</code>. Confirm you were able to log in to the database.</li> </ol>"},{"location":"reference-architectures/automated-password-rotation/#destroy-the-architecture","title":"Destroy the architecture","text":"<pre><code>  cd platform-engineering/reference-architectures/automated-password-rotation/terraform\n\n  terraform init\n  terraform plan -var \"project_id=$PROJECT_ID\"\n  terraform destroy -var \"project_id=$PROJECT_ID\" --auto-approve\n</code></pre>"},{"location":"reference-architectures/automated-password-rotation/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you saw a way to automate password rotation on Google Cloud. First, you saw a generic reference architecture that can be used to automate password rotation in any password management system. In the later section, you saw an example deployment that uses Google Cloud services to rotate password of Cloud Sql database in Google Cloud Secret Manager.</p> <p>Implementing an automatic flow to rotate passwords takes away manual overhead and provide seamless way to tighten your password security. It is recommended to create an automation flow that runs on a regular schedule but can also be easily triggered manually when needed. There can be many variations of this architecture that can be adopted. For example, you can directly trigger a Cloud Run Function from a Google Cloud Scheduler job without sending a message to pub/sub if you don't want to broadcast the password rotation. You should identify a flow that fits your organization requirements and modify the reference architecture to implement it.</p>"},{"location":"reference-architectures/backstage/","title":"Backstage on Google Cloud","text":"<p>A collection of resources related to utilizing Backstage on Google Cloud.</p>"},{"location":"reference-architectures/backstage/#backstage-plugins-for-google-cloud","title":"Backstage Plugins for Google Cloud","text":"<p>A repository for various plugins can be found here -&gt; google-cloud-backstage-plugins</p>"},{"location":"reference-architectures/backstage/#backstage-quickstart","title":"Backstage Quickstart","text":"<p>This is an example deployment of Backstage on Google Cloud with various Google Cloud services providing the infrastructure.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/","title":"Backstage on Google Cloud Quickstart","text":"<p>This quick-start deployment guide can be used to set up an environment to familiarize yourself with the architecture and get an understanding of the concepts related to hosting Backstage on Google Cloud.</p> <p>NOTE: This environment is not intended to be a long lived environment. It is intended for temporary demonstration and learning purposes. You will need to modify the configurations provided to align with your orginazations needs.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#architecture","title":"Architecture","text":"<p>For more information about the architecture, see the Backstage on Google Cloud:Architecture document.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#requirements","title":"Requirements","text":""},{"location":"reference-architectures/backstage/backstage-quickstart/#project","title":"Project","text":"<p>In this guide you can choose to bring your project (BYOP) or have Terraform create a new project for you. The requirements are different based on the option that you choose.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#option-1-bring-your-own-project-byop","title":"Option 1: Bring your own project (BYOP)","text":"<ul> <li>Project ID of a Google Cloud Project</li> <li><code>roles/owner</code> IAM permissions on the project</li> <li>GitHub Personal Access Token, steps to create the token are provided below</li> </ul>"},{"location":"reference-architectures/backstage/backstage-quickstart/#option-2-terraform-managed-project","title":"Option 2: Terraform managed project","text":"<ul> <li>Billing account ID</li> <li>Organization or folder ID</li> <li><code>roles/billing.user</code> IAM permissions on the billing account specified</li> <li><code>roles/resourcemanager.projectCreator</code> IAM permissions on the organization or   folder specified</li> <li>GitHub Personal Access Token, steps to create the token are provided below</li> </ul>"},{"location":"reference-architectures/backstage/backstage-quickstart/#pull-the-source-code","title":"Pull the source code","text":"<p>NOTE: This tutorial is designed to be run from Cloud Shell in the Google Cloud Console.</p> <ul> <li>Clone the repository and change directory to the guide directory</li> </ul> <pre><code>git clone https://github.com/GoogleCloudPlatform/platform-engineering &amp;&amp; \\\ncd platform-engineering/reference-architectures/backstage/backstage-quickstart\n</code></pre> <ul> <li>Set environment variables</li> </ul> <pre><code>export BACKSTAGE_QS_BASE_DIR=$(pwd) &amp;&amp; \\\nsed -n -i -e '/^export BACKSTAGE_QS_BASE_DIR=/!p' -i -e '$aexport  \\\nBACKSTAGE_QS_BASE_DIR=\"'\"${BACKSTAGE_QS_BASE_DIR}\"'\"' ${HOME}/.bashrc\n</code></pre>"},{"location":"reference-architectures/backstage/backstage-quickstart/#project-configuration","title":"Project Configuration","text":"<p>You only need to complete the section for the option that you have selected (either option 1 or 2).</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#option-1-bring-your-own-project-config-byop","title":"Option 1: Bring your own project config (BYOP)","text":"<ul> <li>Set the project environment variables in Cloud Shell</li> </ul> <p>Replace the following values</p> <ul> <li><code>&lt;PROJECT_ID&gt;</code> is the ID of your existing Google Cloud project</li> </ul> <pre><code>export BACKSTAGE_QS_PROJECT_ID=\"&lt;PROJECT_ID&gt;\"\nexport BACKSTAGE_QS_STATE_BUCKET=\"${BACKSTAGE_QS_PROJECT_ID}-terraform\"\n</code></pre> <ul> <li>Set the default <code>gcloud</code> project</li> </ul> <pre><code>gcloud config set project ${BACKSTAGE_QS_PROJECT_ID}\n</code></pre> <ul> <li>Authorize <code>gcloud</code></li> </ul> <pre><code>gcloud auth login --activate --no-launch-browser --quiet --update-adc\n</code></pre> <ul> <li>Create a Cloud Storage bucket to store the Terraform state</li> </ul> <pre><code>gcloud storage buckets create gs://${BACKSTAGE_QS_STATE_BUCKET} --project ${BACKSTAGE_QS_PROJECT_ID}\n</code></pre> <ul> <li>Set the configuration variables</li> </ul> <pre><code>sed -i \"s/YOUR_STATE_BUCKET/${BACKSTAGE_QS_STATE_BUCKET}/g\" ${BACKSTAGE_QS_BASE_DIR}/backend.tf\nsed -i \"s/YOUR_PROJECT_ID/${BACKSTAGE_QS_PROJECT_ID}/g\" ${BACKSTAGE_QS_BASE_DIR}/backstage-qs.auto.tfvars\n</code></pre> <p>You can now the Create the resources.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#option-2-terraform-managed-project-config","title":"Option 2: Terraform managed project config","text":"<ul> <li>Set the configuration variables</li> </ul> <pre><code>nano ${BACKSTAGE_QS_BASE_DIR}/initialize/initialize.auto.tfvars\n</code></pre> <pre><code>environment_name  = \"qs\"\niapUserDomain = \"\"\niapSupportEmail = \"\"\nproject = {\n  billing_account_id = \"XXXXXX-XXXXXX-XXXXXX\"\n  folder_id          = \"############\"\n  name               = \"backstage\"\n  org_id             = \"############\"\n}\n</code></pre> <ul> <li><code>environment_name</code>: the name of the environment (defaults to qs for   quickstart)</li> <li><code>iapUserDomain</code>: the root domain of the GCP Org that the Backstage users will   be in</li> <li><code>iapSupportEmail</code>: support contact for the IAP brand</li> <li><code>project.billing_account_id</code>: the billing account ID</li> <li><code>project.name</code>: the prefix for the display name of the project, the full name   will be <code>&lt;project.name&gt;-&lt;environment_name&gt;</code></li> </ul> <p>Enter either <code>project.folder_id</code> OR <code>project.org_id</code></p> <ul> <li><code>project.folder_id</code>: the Google Cloud folder ID</li> <li> <p><code>project.org_id</code>: the Google Cloud organization ID</p> </li> <li> <p>Authorize <code>gcloud</code></p> </li> </ul> <pre><code>gcloud auth login --activate --no-launch-browser --quiet --update-adc\n</code></pre> <ul> <li>Create a new project</li> </ul> <pre><code>cd ${BACKSTAGE_QS_BASE_DIR}/initialize\nterraform init &amp;&amp; \\\nterraform plan -input=false -out=tfplan &amp;&amp; \\\nterraform apply -input=false tfplan &amp;&amp; \\\nrm tfplan &amp;&amp; \\\nterraform init -force-copy -migrate-state &amp;&amp; \\\nrm -rf state\n</code></pre> <ul> <li>Set the project environment variables in Cloud Shell</li> </ul> <pre><code>BACKSTAGE_QS_PROJECT_ID=$(grep environment_project_id \\\n${BACKSTAGE_QS_BASE_DIR}/backstage-qs.auto.tfvars | awk -F\"=\" '{print $2}' | xargs)\n</code></pre> <p>You can now the Create the resources.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#create-the-resources","title":"Create the resources","text":"<p>Before running Terraform, make sure that the Service Usage API and Service Management API are enabled.</p> <ul> <li>Enable Service Usage API</li> </ul> <pre><code>gcloud services enable serviceusage.googleapis.com\n</code></pre> <ul> <li>Enable Service Management API</li> </ul> <pre><code>gcloud services enable servicemanagement.googleapis.com\n</code></pre> <ul> <li>In situations where you have run this quickstart before and then cleaned-up   the resources but are re-using the project, it might be neccasary to restore   the endpoints from a deleted state first.</li> </ul> <pre><code>BACKSTAGE_QS_PREFIX=$(grep environment_name \\\n${BACKSTAGE_QS_BASE_DIR}/backstage-qs.auto.tfvars | awk -F\"=\" '{print $2}' | xargs)\nBACKSTAGE_QS_PROJECT_ID=$(grep environment_project_id \\\n${BACKSTAGE_QS_BASE_DIR}/backstage-qs.auto.tfvars | awk -F\"=\" '{print $2}' | xargs)\ngcloud endpoints services undelete \\\n${BACKSTAGE_QS_PREFIX}.endpoints.${BACKSTAGE_QS_PROJECT_ID}.cloud.goog \\\n--quiet 2&gt;/dev/null\n</code></pre> <ul> <li>Create the resources</li> </ul> <pre><code>cd ${BACKSTAGE_QS_BASE_DIR} &amp;&amp; \\\nterraform init &amp;&amp; \\\nterraform plan -input=false -out=tfplan &amp;&amp; \\\nterraform apply -input=false tfplan &amp;&amp; \\\nrm tfplan\n</code></pre> <p>This will take a while to create all of the required resources, figure somewhere between 15 and 20 minutes.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#cleanup","title":"Cleanup","text":""},{"location":"reference-architectures/backstage/backstage-quickstart/#resources","title":"Resources","text":"<ul> <li>Destroy the resources</li> </ul> <pre><code>cd ${BACKSTAGE_QS_BASE_DIR} &amp;&amp; \\\nterraform init &amp;&amp; \\\nterraform destroy -auto-approve &amp;&amp; \\\nrm -rf .terraform .terraform.lock.hcl\n</code></pre>"},{"location":"reference-architectures/backstage/backstage-quickstart/#project-deletion","title":"Project Deletion","text":"<p>You only need to complete the section for the option that you have selected.</p>"},{"location":"reference-architectures/backstage/backstage-quickstart/#option-1-bring-your-own-project-deletion-byop","title":"Option 1: Bring your own project deletion (BYOP)","text":"<ul> <li>Delete the project</li> </ul> <pre><code>gcloud projects delete ${BACKSTAGE_QS_PROJECT_ID}\n</code></pre>"},{"location":"reference-architectures/backstage/backstage-quickstart/#option-2-terraform-managed-project-deletion","title":"Option 2: Terraform managed project deletion","text":"<ul> <li>Destroy the project</li> </ul> <pre><code>cd ${BACKSTAGE_QS_BASE_DIR}/initialize &amp;&amp; \\\nTERRAFORM_BUCKET_NAME=$(grep bucket backend.tf | awk -F\"=\" '{print $2}' |\nxargs) &amp;&amp; \\\ncp backend.tf.local backend.tf &amp;&amp; \\\nterraform init -force-copy -lock=false -migrate-state &amp;&amp; \\\ngsutil -m rm -rf gs://${TERRAFORM_BUCKET_NAME}/* &amp;&amp; \\\nterraform init &amp;&amp; \\\nterraform destroy -auto-approve  &amp;&amp; \\\nrm -rf .terraform .terraform.lock.hcl state/\n</code></pre>"},{"location":"reference-architectures/backstage/backstage-quickstart/#environment-configuration","title":"Environment configuration","text":"<ul> <li>Remove Terraform files and temporary files</li> </ul> <pre><code>cd ${BACKSTAGE_QS_BASE_DIR} &amp;&amp; \\\nrm -rf \\\n.terraform \\\n.terraform.lock.hcl \\\ninitialize/.terraform \\\ninitialize/.terraform.lock.hcl \\\ninitialize/backend.tf.local \\\ninitialize/state\n</code></pre> <ul> <li>Reset the TF variables file</li> </ul> <pre><code>cd ${BACKSTAGE_QS_BASE_DIR} &amp;&amp; \\\ncp backstage-qs-auto.tfvars.local backstage-qs.auto.tfvars\n</code></pre> <ul> <li>Remove the environment variables</li> </ul> <pre><code>sed \\\n-i -e '/^export BACKSTAGE_QS_BASE_DIR=/d' \\\n${HOME}/.bashrc\n</code></pre>"},{"location":"reference-architectures/backstage/backstage-quickstart/docs/backstage-quickstart-architecture/","title":"TODO","text":""},{"location":"reference-architectures/cloud_deploy_flow/","title":"Platform Engineering Deployment Demo","text":""},{"location":"reference-architectures/cloud_deploy_flow/#background","title":"Background","text":"<p>Platform engineering focuses on providing a robust framework for managing the deployment of applications across various environments. One of the critical components in this field is the automation of application deployments, which streamlines the entire process from development to production.</p> <p>Most organizations have predefined rules around security, privacy, deployment, and change management to ensure consistency and compliance across environments. These rules often include automated security scans, privacy checks, and controlled release protocols that track all changes in both production and pre-production environments.</p> <p>In this demo, the architecture is designed to show how a deployment tool like Cloud Deploy can integrate smoothly into such workflows, supporting both automation and oversight. The process starts with release validation, ensuring that only compliant builds reach the release stage. Rollout approvals then offer flexibility, allowing teams to implement either manual checks or automated responses depending on specific requirements.</p> <p>This setup provides a blueprint for organizations to streamline deployment cycles while maintaining robust governance. By using this demo, you can see how these components work together, from container build through deployment, in a way that minimizes disruption to existing processes and aligns with typical organizational change management practices.</p> <p>This demo showcases a complete workflow that begins with the build of a container and progresses through various stages, ultimately resulting in the deployment of a new application.</p>"},{"location":"reference-architectures/cloud_deploy_flow/#overview-of-the-demo","title":"Overview of the Demo","text":"<p>This demo illustrates the end-to-end deployment process, starting from the container build phase. Here's a high-level overview of the workflow:</p> <ol> <li> <p>Container Build Process: The demo begins when a container is built-in     Cloud Build. Upon completion, a notification is sent to a Pub/Sub message     queue.</p> </li> <li> <p>Release Logic: A Cloud Run Function subscribes to this message queue,     assessing whether a release should be created. If a release is warranted, a     message is sent to a \"Command Queue\" (another Pub/Sub topic).</p> </li> <li> <p>Creating a Release: A dedicated function listens to the \"Command Queue\"     and communicates with Cloud Deploy to create a new release. Once the release     is created, a notification is dispatched to the Pub/Sub Operations topic.</p> </li> <li> <p>Rollout Process: Another Cloud Function picks up this notification and     initiates the rollout process by sending a <code>createRolloutRequest</code> to the     \"Command Queue.\"</p> </li> <li> <p>Approval Process: Since rollouts typically require approval, a     notification is sent to the <code>cloud-deploy-approvals</code> Pub/Sub queue. An     approval function then picks up this message, allowing you to implement your     custom logic or utilize the provided site Demo to return JSON, such as     <code>{ \"manualApproval\": \"true\" }</code>.</p> </li> <li> <p>Deployment: Once approved, the rollout proceeds, and the new application     is deployed.</p> </li> </ol> <p></p>"},{"location":"reference-architectures/cloud_deploy_flow/#prerequisites","title":"Prerequisites","text":"<ul> <li>A GCP project with billing enabled</li> <li>The following APIs must be enabled in your GCP project:<ul> <li><code>compute.googleapis.com</code></li> <li><code>iam.googleapis.com</code></li> <li><code>cloudresourcemanager.googleapis.com</code></li> </ul> </li> <li>Ensure you have the necessary IAM roles to manage these resources.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/#iam-roles-used-by-terraform","title":"IAM Roles used by Terraform","text":"<p>To run this demo, the following IAM roles will be granted to the service account created by the Terraform configuration:</p> <ul> <li><code>roles/iam.serviceAccountUser</code>: Allows management of service accounts.</li> <li><code>roles/logging.logWriter</code>: Grants permission to write logs.</li> <li><code>roles/artifactregistry.writer</code>: Enables writing to Artifact Registry.</li> <li><code>roles/storage.objectUser</code>: Provides access to Cloud Storage objects.</li> <li><code>roles/clouddeploy.jobRunner</code>: Allows execution of Cloud Deploy jobs.</li> <li><code>roles/clouddeploy.releaser</code>: Grants permissions to release configurations in   Cloud Deploy.</li> <li><code>roles/run.developer</code>: Enables deploying and managing Cloud Run services.</li> <li><code>roles/cloudbuild.builds.builder</code>: Allows triggering and managing Cloud Build   processes.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/#gcp-services-enabled-by-terraform","title":"GCP Services enabled by Terraform","text":"<p>The following Google Cloud services must be enabled in your project to run this demo:</p> <ul> <li><code>pubsub.googleapis.com</code>: Enables Pub/Sub for messaging between services.</li> <li><code>clouddeploy.googleapis.com</code>: Allows use of Cloud Deploy for managing   deployments.</li> <li><code>cloudbuild.googleapis.com</code>: Enables Cloud Build for building and deploying   applications.</li> <li><code>compute.googleapis.com</code>: Provides access to Compute Engine resources.</li> <li><code>cloudresourcemanager.googleapis.com</code>: Allows management of project-level   permissions and resources.</li> <li><code>run.googleapis.com</code>: Enables Cloud Run for deploying and running   containerized applications.</li> <li><code>cloudfunctions.googleapis.com</code>: Allows use of Cloud Functions for   event-driven functions.</li> <li><code>eventarc.googleapis.com</code>: Enables Eventarc for routing events from sources to   targets.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/#getting-started","title":"Getting Started","text":"<p>To run this demo, follow these steps:</p> <ol> <li> <p>Fork and Clone the Repository: Start by forking this repository to your     GitHub account (So you can connect GCP to this repository), then clone it to     your local environment. After cloning, change your directory to the     deployment demo:</p> <pre><code>cd platform-engineering/reference-architectures/cloud_deploy_flow\n</code></pre> </li> <li> <p>Set Up Environment Variables or Variables File: You can set the     necessary variables either by exporting them as environment variables or by     creating a <code>terraform.tfvars</code> file. Refer to <code>variables.tf</code> for more details     on each variable. Ensure the values match your Google Cloud project and     GitHub configuration.</p> <ul> <li> <p>Option 1: Set environment variables manually in your shell:</p> <pre><code>export TF_VAR_project_id=\"your-google-cloud-project-id\"\nexport TF_VAR_region=\"your-preferred-region\"\nexport TF_VAR_github_owner=\"your-github-repo-owner\"\nexport TF_VAR_github_repo=\"your-github-repo-name\"\n</code></pre> </li> <li> <p>Option 2: Create a <code>terraform.tfvars</code> file in the same directory as   your Terraform configuration and populate it with the following:</p> <pre><code>project_id  = \"your-google-cloud-project-id\"\nregion      = \"your-preferred-region\"\ngithub_owner = \"your-github-repo-owner\"\ngithub_repo = \"your-github-repo-name\"\n</code></pre> </li> </ul> </li> <li> <p>Initialize and Apply Terraform: With the environment variables set,     initialize and apply the Terraform configuration:</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>Note: Applying Terraform may take a few minutes as it creates the necessary resources.</p> </li> <li> <p>Connect GitHub Repository to Cloud Build: Due to occasional issues with     automatic connections, you may need to manually attach your GitHub     repository to Cloud Build in the Google Cloud Console.</p> <p>If you get the following error you will need to manually connect your repository to the project:</p> <pre><code>Error: Error creating Trigger: googleapi: Error 400: Repository mapping does\nnot exist.\n</code></pre> <p>Re-run step 3 to ensure all resources are deployed</p> </li> <li> <p>Navigate to the Demo site: Once the Terraform setup is complete, switch     to the Demo site directory:</p> <pre><code>cd platform-engineering/reference-architectures/cloud-deploy-flow/WebsiteDemo\n</code></pre> </li> <li> <p>Authenticate and Run the Demo site:</p> <ul> <li> <p>Ensure you are running these commands on a local machine or a machine   with GUI/web browser access, as Cloud Shell may not fully support   running the demo site.</p> </li> <li> <p>Set your Google Cloud project by running:</p> <pre><code>gcloud config set project &lt;your_project_id&gt;\n</code></pre> </li> <li> <p>Authenticate your Google Cloud CLI session:</p> <pre><code>gcloud auth application-default login\n</code></pre> </li> <li> <p>Install required npm packages and start the demo site:</p> <pre><code>npm install\nnode index.js\n</code></pre> </li> <li> <p>Open <code>http://localhost:8080</code> in your browser to observe the demo site in   action.</p> </li> </ul> </li> <li> <p>Trigger a Build in Cloud Build:</p> <ul> <li>Initiate a build in Cloud Build. As the build progresses, messages will   display on the demo site, allowing you to follow each step in the   deployment process.</li> <li>You can also monitor the deployment rollout on   Google Cloud Console.</li> </ul> </li> <li> <p>Approve the Rollout: When an approval message is received, you\u2019ll need     to send a response to complete the deployment. Use the message data provided     and add a <code>ManualApproval</code> field:</p> <pre><code>{\n    \"message\": {\n    \"data\": \"&lt;base64-encoded data&gt;\",\n    \"attributes\": {\n        \"Action\": \"Required\",\n        \"Rollout\": \"rollout-123\",\n        \"ReleaseId\": \"release-456\",\n        \"ManualApproval\": \"true\"\n    }\n    }\n}\n</code></pre> </li> <li> <p>Verify the Deployment: Once the approval is processed, the deployment     should finish rolling out. Check the Cloud Deploy dashboard in the Google     Cloud Console to confirm the deployment status.</p> </li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/#conclusion","title":"Conclusion","text":"<p>This demo encapsulates the essential components and workflow for deploying applications using platform engineering practices. It illustrates how various services interact to ensure a smooth deployment process.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/","title":"Cloud Deployment Approvals with Pub/Sub","text":"<p>This project provides a Google Cloud Run Function to automate deployment approvals based on messages received via Google Cloud Pub/Sub. The function processes deployment requests, checks conditions for rollout approval, and publishes an approval command if the requirements are met.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#usage-note","title":"Usage Note","text":"<p>This code is designed to integrate with the Terraform configuration for the cloud_deploy_flow demo. While you can deploy this component individually, it's primarily intended to be used as part of the full Terraform-managed workflow. Please note that this section of the readme may be less actively maintained, as the preferred deployment method relies on the Terraform setup.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#features","title":"Features","text":"<ul> <li>Listens to Pub/Sub messages for deployment approvals</li> <li>Validates deployment conditions (manual approval, rollout ID, etc.)</li> <li>Publishes approval commands to another Pub/Sub topic if conditions are met</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#setup","title":"Setup","text":""},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#requirements","title":"Requirements","text":"<ul> <li>POSIX compliant Bash Shell</li> <li>Go 1.16 or later</li> <li>Google Cloud SDK</li> <li>Access to Google Cloud Pub/Sub</li> <li>Environment variables to configure project details</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone &lt;repository-url&gt;\ncd &lt;repository-folder&gt;\n</code></pre> </li> <li> <p>Enable APIs: Enable the Google Cloud Pub/Sub and Deploy APIs for your     project:</p> <pre><code>gcloud services enable pubsub.googleapis.com deploy.googleapis.com\n</code></pre> </li> <li> <p>Deploy the Function: Use Google Cloud SDK to deploy the function:</p> <pre><code>gcloud functions deploy cloudDeployApprovals --runtime go116 \\\n--trigger-event-type google.cloud.pubsub.topic.v1.messagePublished \\\n--trigger-resource YOUR_SUBSCRIBE_TOPIC\n</code></pre> </li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#environment-variables","title":"Environment Variables","text":"<p>The function relies on environment variables to specify project configuration. Ensure these are set before deploying the function:</p> Variable Name Description Required <code>PROJECTID</code> Google Cloud project ID Yes <code>LOCATION</code> The deployment location (region) Yes <code>SENDTOPICID</code> Pub/Sub topic ID for sending commands Yes"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#code-structure","title":"Code Structure","text":"<ul> <li> <p>config struct: Holds configuration for the environment variables.</p> </li> <li> <p>PubsubMessage and ApprovalsData structs: Define the structure of messages   received from Pub/Sub and attributes within them.</p> </li> <li> <p>cloudDeployApprovals function: Entry point for handling messages.   Validates the conditions and, if met, triggers the <code>sendCommandPubSub</code>   function to send an approval command.</p> </li> <li> <p>sendCommandPubSub function: Publishes a command message to the Pub/Sub   topic to approve a deployment rollout.</p> </li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#usage","title":"Usage","text":"<p>The function <code>cloudDeployApprovals</code> is invoked whenever a message is published to the configured Pub/Sub topic. Upon receiving a message, the function will:</p> <ol> <li>Parse and validate the message.</li> <li>Check if the action is <code>Required</code>, if a rollout ID is provided, and if     manual approval is marked as \"true.\"</li> <li>If conditions are met, it will publish an approval command to the     <code>SENDTOPICID</code> topic.</li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#sample-pubsub-message","title":"Sample Pub/Sub Message","text":"<p>A message sent to the function should resemble this JSON structure:</p> <pre><code>{\n  \"message\": {\n    \"data\": \"&lt;base64-encoded data&gt;\",\n    \"attributes\": {\n      \"Action\": \"Required\",\n      \"Rollout\": \"rollout-123\",\n      \"ReleaseId\": \"release-456\",\n      \"ManualApproval\": \"true\"\n    }\n  }\n}\n</code></pre>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#custom-manual-approval-field","title":"Custom Manual Approval Field","text":"<p>In the <code>ApprovalsData</code> struct, there is a <code>ManualApproval</code> field. This field is a custom addition, not provided by Google Cloud Deploy, and serves as a placeholder for an external approval system.</p> <p>To integrate the approval system, you can replace or adapt this field to suit your existing change process workflow. For instance, you could link this field to an external ticketing or project management system to track and verify approvals. Implementing an approval system allows greater control over deployment rollouts, ensuring they align with your organization\u2019s policies.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployApprovals/#logging","title":"Logging","text":"<p>The function logs each major step, from invocation to message processing and condition checking, to facilitate debugging and monitoring.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/","title":"Cloud Deploy Interactions with Pub/Sub","text":"<p>This project demonstrates a Google Cloud Run Function to manage deployments by creating releases, rollouts, or approving rollouts based on incoming Pub/Sub messages. The function leverages Google Cloud Deploy and listens for deployment-related commands sent via Pub/Sub, executing appropriate actions based on the command type.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#usage-note","title":"Usage Note","text":"<p>This code is designed to integrate with the Terraform configuration for the cloud_deploy_flow demo. While you can deploy this component individually, it's primarily intended to be used as part of the full Terraform-managed workflow. Please note that this section of the readme may be less actively maintained, as the preferred deployment method relies on the Terraform setup.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#features","title":"Features","text":"<ul> <li> <p>Listens for Pub/Sub messages with deployment commands (CreateRelease,   CreateRollout, ApproveRollout) Messages should include protobuf request.</p> </li> <li> <p>Initiates Google Cloud Deploy actions based on the received command.</p> </li> <li> <p>Logs each step of the deployment process for better traceability.</p> </li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#setup","title":"Setup","text":""},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#requirements","title":"Requirements","text":"<ul> <li>Go 1.16 or later</li> <li>Google Cloud SDK</li> <li>Access to Google Cloud Deploy and Pub/Sub</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone &lt;repository-url&gt;\ncd &lt;repository-folder&gt;\n</code></pre> </li> <li> <p>Set up Google Cloud: Ensure you have enabled the Google Cloud Deploy and     Pub/Sub APIs in your project.</p> </li> <li> <p>Deploy the Function: Deploy the function using Google Cloud SDK:</p> <pre><code>gcloud functions deploy cloudDeployInteractions --runtime go116 \\\n--trigger-event-type google.cloud.pubsub.topic.v1.messagePublished \\\n--trigger-resource YOUR_TOPIC_NAME\n</code></pre> </li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#pubsub-message-format","title":"Pub/Sub Message Format","text":"<p>The Pub/Sub message should include a JSON payload with a <code>command</code> field specifying the type of deployment action to execute. Examples of the command types include:</p> <ul> <li><code>CreateRelease</code>: Creates a new release for deployment.</li> <li><code>CreateRollout</code>: Initiates a rollout of the release.</li> <li><code>ApproveRollout</code>: Approves a pending rollout.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#sample-pubsub-message","title":"Sample Pub/Sub Message","text":"<p>The message should follow this structure:</p> <pre><code>{\n  \"message\": {\n    \"data\": \"&lt;base64-encoded JSON containing command data&gt;\"\n  }\n}\n</code></pre> <p>The JSON inside <code>data</code> should follow the format for <code>DeployCommand</code>:</p> <pre><code>{\n  \"command\": \"CreateRelease\",\n  \"createReleaseRequest\": {\n    // Release creation parameters\n  },\n  \"createRolloutRequest\": {\n    // Rollout creation parameters\n  },\n  \"approveRolloutRequest\": {\n    // Rollout approval parameters\n  }\n}\n</code></pre>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#code-structure","title":"Code Structure","text":"<ul> <li> <p>DeployCommand struct: Defines the command to be executed and the   parameters for each deploy action (create release, create rollout, or approve   rollout).</p> </li> <li> <p>cloudDeployInteractions function: Main function triggered by Pub/Sub   messages. It parses the message and calls the respective deployment function   based on the command.</p> </li> <li> <p>cdCreateRelease: Creates a release in Google Cloud Deploy.</p> </li> <li>cdCreateRollout: Initiates a rollout for a specified release.</li> <li>cdApproveRollout: Approves an existing rollout.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployInteractions/#logging","title":"Logging","text":"<p>Each function logs key steps, from initialization to message handling and completion of deployments, helping in troubleshooting and monitoring.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/","title":"Cloud Deploy Operations Function","text":"<p>This project contains a Google Cloud Run Function written in Go, designed to interact with Google Cloud Deploy. The function listens for deployment events on a Pub/Sub topic, processes those events, and triggers specific deployment operations based on the event details. For instance, when a deployment release succeeds, it triggers a rollout creation and sends the relevant command to another Pub/Sub topic.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#usage-note","title":"Usage Note","text":"<p>This code is designed to integrate with the Terraform configuration for the cloud_deploy_flow demo. While you can deploy this component individually, it's primarily intended to be used as part of the full Terraform-managed workflow. Please note that this section of the readme may be less actively maintained, as the preferred deployment method relies on the Terraform setup.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#requirements","title":"Requirements","text":"<ul> <li>Go 1.20 or later</li> <li>Google Cloud SDK</li> <li>Google Cloud Pub/Sub</li> <li>Google Cloud Deploy API</li> <li>Set environment variables for Google Cloud project configuration</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#environment-variables","title":"Environment Variables","text":"<p>The function relies on environment variables to specify project configuration. Ensure these are set before deploying the function:</p> Variable Name Description Required <code>PROJECTID</code> Google Cloud project ID Yes <code>LOCATION</code> The deployment location (region) Yes <code>SENDTOPICID</code> Pub/Sub topic ID for sending commands Yes"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#structure","title":"Structure","text":""},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#main-components","title":"Main Components","text":"<ul> <li>config: Stores the environment configuration necessary for the function.</li> <li>PubsubMessage: Structure representing a message from Pub/Sub, with <code>Data</code>   payload and <code>Attributes</code> metadata.</li> <li>OperationsData: Metadata that describes deployment action and resource   details.</li> <li>CommandMessage: Structure for deployment commands, like <code>CreateRollout</code>.</li> <li>cloudDeployOperations: Main Cloud Run Function triggered by a deployment   event, processes release successes to initiate rollouts.</li> <li>sendCommandPubSub: Publishes a <code>CommandMessage</code> to a specified Pub/Sub   topic, which triggers deployment operations.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#function-workflow","title":"Function Workflow","text":"<ol> <li>Trigger: The function <code>cloudDeployOperations</code> is triggered by a     deployment event, specifically a CloudEvent.</li> <li>Event Parsing: The function parses the event data into a <code>Message</code>     struct, checking for deployment success events.</li> <li>Rollout Creation: If a release success is detected, it creates a     <code>CommandMessage</code> for a rollout and calls <code>sendCommandPubSub</code>.</li> <li>Command Publish: The <code>sendCommandPubSub</code> function publishes the     <code>CommandMessage</code> to a designated Pub/Sub topic to initiate the rollout.</li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#setup-and-deployment","title":"Setup and Deployment","text":""},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#local-development","title":"Local Development","text":"<ol> <li>Clone the repository and set up your local environment with the necessary     environment variables.</li> <li>Run the Cloud Run Functions framework locally to test the function:</li> </ol> <pre><code>functions-framework --target=cloudDeployOperations\n</code></pre>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#deployment-to-google-cloud-run-functions","title":"Deployment to Google Cloud Run Functions","text":"<ol> <li> <p>Set up your Google Cloud environment and enable the necessary APIs:</p> <pre><code>gcloud services enable cloudfunctions.googleapis.com pubsub.googleapis.com\nclouddeploy.googleapis.com\n</code></pre> </li> <li> <p>Deploy the function to Google Cloud:</p> <pre><code>gcloud functions deploy cloudDeployOperations \\\n   --runtime go120 \\\n   --trigger-topic &lt;YOUR_TRIGGER_TOPIC&gt; \\\n   --set-env-vars PROJECTID=&lt;YOUR_PROJECT_ID&gt;,LOCATION=&lt;YOUR_LOCATION&gt;,SENDTOPICID=&lt;YOUR_SEND_TOPIC_ID&gt;\n</code></pre> </li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#error-handling","title":"Error Handling","text":"<ul> <li>If message parsing fails, the function logs an error but acknowledges the   message to prevent retries.</li> <li>Command failures are logged, and the function acknowledges the message to   prevent reprocessing of erroneous commands.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the <code>LICENSE</code> file for details.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/cloudDeployOperations/#notes","title":"Notes","text":"<ul> <li>For production environments, consider validating that the <code>TargetId</code> within   <code>CommandMessage</code> is dynamically populated based on actual Pub/Sub message   data.</li> <li>The function relies on <code>pubsub.NewClient</code> which should be carefully monitored   in production for connection management.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/","title":"Example Cloud Run Function","text":"<p>This project demonstrates a Google Cloud Run Function that triggers deployments based on Pub/Sub messages. The function listens for build notifications from Google Cloud Build and initiates a release in Google Cloud Deploy when a build succeeds.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/#usage-note","title":"Usage Note","text":"<p>This code is designed to integrate with the Terraform configuration for the cloud_deploy_flow demo. While you can deploy this component individually, it's primarily intended to be used as part of the full Terraform-managed workflow. Please note that this section of the readme may be less actively maintained, as the preferred deployment method relies on the Terraform setup.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Env</li> <li>Function Overview</li> <li>Deploying the Function</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go version 1.15 or later</li> <li>Google Cloud account</li> <li>Google Cloud SDK installed and configured</li> <li>Necessary permissions for Cloud Build and Cloud Deploy</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/#environment-variables","title":"Environment Variables","text":"<p>The function relies on environment variables to specify project configuration. Ensure these are set before deploying the function:</p> Variable Name Description Required <code>PROJECTID</code> Google Cloud project ID Yes <code>LOCATION</code> The deployment location (region) Yes <code>PIPELINE</code> The name of the delivery pipeline in Cloud Deploy. Yes <code>TRIGGER</code> The ID of the build trigger in Cloud Build. Yes <code>SENDTOPICID</code> Pub/Sub topic ID for sending commands Yes"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/#function-overview","title":"Function Overview","text":"<p>The <code>deployTrigger</code> function is invoked by Pub/Sub events. Here's a breakdown of its key components:</p> <ol> <li> <p>Initialization:</p> <ul> <li>Loads environment variables into a configuration struct.</li> <li>Registers the function to be triggered by CloudEvents.</li> </ul> </li> <li> <p>Message Handling:</p> <ul> <li>Parses incoming Pub/Sub messages.</li> <li>Validates build notifications based on specified criteria (trigger ID and   build status).</li> </ul> </li> <li> <p>Release Creation:</p> <ul> <li>Extracts relevant image information from the build notification.</li> <li>Constructs a <code>CreateReleaseRequest</code> for Cloud Deploy.</li> <li>Sends the request to the specified Pub/Sub topic.</li> </ul> </li> <li> <p>Random ID Generation:</p> <ul> <li>Generates a unique release ID to ensure each deployment is distinct.</li> </ul> </li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudFunctions/createRelease/#deploying-the-function","title":"Deploying the Function","text":"<p>To deploy the function, follow these steps:</p> <ol> <li>Ensure that your Google Cloud SDK is authenticated and configured with the     correct project.</li> <li>Use the following command to deploy the function:</li> </ol> <pre><code>gcloud functions deploy deployTrigger \\\n    --runtime go113 \\\n    --trigger-topic YOUR_TOPIC_NAME \\\n    --env-file .env\n</code></pre>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/","title":"Random Date Service","text":"<p>This repository contains a sample application designed to demonstrate how deployments can work through Google Cloud Deploy and Cloud Build. Instead of a traditional \"Hello World\" application, this project generates and serves a random date, showcasing how to set up a cloud-based service.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#usage-note","title":"Usage Note","text":"<p>This code is designed to integrate with the Terraform configuration for the cloud_deploy_flow demo. While you can deploy this component individually, it's primarily intended to be used as part of the full Terraform-managed workflow. Please note that this section of the readme may be less actively maintained, as the preferred deployment method relies on the Terraform setup.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#overview","title":"Overview","text":"<p>The <code>Random Date Service</code> is built to illustrate the process of deploying an application using Cloud Run and Cloud Deploy. The application serves a random date formatted as a string. This simple service allows you to explore key concepts in cloud deployment without the complexity of a full-fledged application.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#components","title":"Components","text":""},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#1-maingo","title":"1. main.go","text":"<p>This is the core of the application, where the HTTP server is defined. It handles requests and responds with a randomly generated date.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#2-dockerfile","title":"2. Dockerfile","text":"<p>The Dockerfile specifies how to build a container image for the application. This image will be used in Cloud Run for deploying the service.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#3-skaffoldyaml","title":"3. skaffold.yaml","text":"<p>This file is configured for Google Cloud Deploy, facilitating the deployment process by managing builds and configurations in a single file.</p>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#4-runyaml","title":"4. run.yaml","text":"<p>The <code>run.yaml</code> file defines the configuration for Cloud Run and Cloud Deploy. Key aspects to note include:</p> <ul> <li>Service Name: This defines the name of the service as   <code>random-date-service</code>.</li> <li>Image Specification: The <code>image</code> field under <code>spec</code> is set to <code>pizza</code>.   This is crucial, as it indicates to Cloud Deploy where to substitute the   image. This substitution occurs based on the <code>createRelease</code> function in   <code>main.go</code>, specifically noted on line 122.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#usage","title":"Usage","text":"<p>To deploy and test this application:</p> <ol> <li>Build the Docker Image: Use the provided Dockerfile to create a     container image.</li> <li>Deploy to Cloud Run: Utilize the <code>run.yaml</code> configuration to deploy the     service.</li> <li>Monitor Deployments: Use Cloud Deploy to observe the deployment pipeline     and ensure the service is running as expected.</li> <li>Access the Service: After deployment, access the service through its     endpoint to receive a random date.</li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/CloudRun/#conclusion","title":"Conclusion","text":"<p>This sample application serves as a foundational example of how to leverage cloud services for deploying applications. By utilizing Google Cloud Deploy and Cloud Build, you can understand the deployment lifecycle and how cloud-native applications can be effectively managed and served.</p> <p>Feel free to explore the code and configurations provided in this repository to get a better grasp of the deployment process.</p>"},{"location":"reference-architectures/cloud_deploy_flow/WebsiteDemo/","title":"Pub/Sub Local Demo","text":"<p>This project is a simple demonstration of a Pub/Sub system using Google Cloud Pub/Sub and a basic Express.js server. It is designed to visually understand how messages are sent to and from Pub/Sub queues. The code provided is primarily for demonstration purposes and is not intended for production use.</p>"},{"location":"reference-architectures/cloud_deploy_flow/WebsiteDemo/#features","title":"Features","text":"<ul> <li>Real-time Message Display: Messages from various Pub/Sub subscriptions are   fetched and displayed in a web interface.</li> <li>Clear Messages: Users can clear all displayed messages from the UI.</li> <li>Send Messages: Users can send messages to the Pub/Sub topic via the input   area.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/WebsiteDemo/#project-structure","title":"Project Structure","text":"<ul> <li>public/index.html: The main HTML file that provides the structure for the   web interface.</li> <li>public/script.js: Contains JavaScript logic for fetching messages, sending   new messages, and clearing messages.</li> <li>public/styles.css: Contains styling for the web interface to ensure a   clean and responsive layout.</li> <li>index.js: The main server file that handles Pub/Sub operations and serves   static files.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/WebsiteDemo/#installation","title":"Installation","text":"<ol> <li> <p>Install the required dependencies:</p> <p>npm install</p> </li> <li> <p>Set your Google Cloud project ID and subscription names in the <code>index.js</code>     file.</p> </li> <li> <p>Start the server:</p> <p>node index.js</p> </li> <li> <p>Open your web browser and go to <code>http://localhost:8080</code> to access the demo.</p> </li> </ol>"},{"location":"reference-architectures/cloud_deploy_flow/WebsiteDemo/#usage","title":"Usage","text":"<ul> <li>Sending Messages: Enter a message in the textarea and click \"Submit\" to   send it to the Pub/Sub topic.</li> <li>Viewing Messages: The messages received from the Pub/Sub subscriptions   will be displayed in their respective boxes.</li> <li>Clearing Messages: Click the \"Clear\" button to remove all messages from   the display.</li> </ul>"},{"location":"reference-architectures/cloud_deploy_flow/WebsiteDemo/#disclaimer","title":"Disclaimer","text":"<p>This code is intended for educational and demonstration purposes only. It may not be suitable for production environments due to lack of error handling, security considerations, and scalability.</p>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/","title":"Gemini-powered migration blocker analysis","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/#deploy-and-run-the-application","title":"Deploy and run the application","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/#google-cloud","title":"Google Cloud","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/#docker-compose","title":"Docker Compose","text":"<p>Pre-requisites:</p> <ul> <li>Docker. Tested with version <code>28.0.4</code></li> <li>Docker Compose. Tested with version <code>v2.34.0</code></li> </ul> <p>To run this application using Docker Compose, you do the following:</p> <ol> <li> <p>Open your shell.</p> </li> <li> <p>Run the application using Docker Compose</p> <pre><code>UID=\"$(id -u)\" GID=\"$(id -g)\" docker compose up --build --renew-anon-volumes\n</code></pre> </li> </ol>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/","title":"README.md","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/#intro","title":"Intro","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/#running-locally-in-development","title":"Running Locally in development","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/#env","title":".env","text":"<p>Create a .env file in the root of the project with the following contents, maksing sure to replace the placeholder text:</p> <pre><code>GOOGLE_CLOUD_PROJECT=\"&lt;your-gcp-project&gt;\"\nGCS_BUCKET_NAME=\"&lt;bucket-name-for-docs&gt;\"\nGEMINI_MODEL_NAME=\"gemini-2.0-flash\"\nGCP_LOCATION=\"&lt;region&gt;\"\n</code></pre>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/#prereqs-dependencies","title":"Prereqs / Dependencies","text":"<p>Prereqs include:</p> <ul> <li>A Google Cloud Project and credentials with access to the API's listed below</li> <li>A working python environemnt, tested with 3.12.3</li> </ul> <p>This project uses uv for python environment and dependency management.</p> <p>To get setup with <code>uv</code> after cloning the repo :</p> <pre><code>uv sync\nsource .venv/bin/activate\n</code></pre> <p>We provide a requirements.txt file for those that require <code>pip</code> , we generate it automatically with :</p> <pre><code> uv export --format requirements-txt --no-hashes &gt; requirements.txt\n</code></pre> <p>Environment management via <code>pip</code> is left to the user</p> <p>Google Cloud API access is handled via ADC , so you can either use the gcloud CLI if running locally :</p> <pre><code>export GOOGLE_CLOUD_PROJECT=&lt;your-gcp-project&gt;\ngcloud config set project $GOOGLE_CLOUD_PROJECT\ngcloud auth application-default login\n</code></pre> <p>or use a service account if your environemnt supports it.</p> <p>Enable the required API's in Google Cloud :</p> <pre><code>export GOOGLE_CLOUD_PROJECT=&lt;your-gcp-project&gt;\ngcloud config set project $GOOGLE_CLOUD_PROJEC\ngcloud services enable firestore.googleapis.com\ngcloud services enable datastore.googleapis.com\ngcloud services enable aiplatform.googleapis.com\ngcloud services enable storage.googleapis.com\n</code></pre> <p>Now you can start the development server with :</p> <pre><code>fastapi dev main.py\n</code></pre> <p>The development server will also host OpenAPI docs for the service at : [http://127.0.0.1:8000/docs]</p> <p>Once running , you can either test from that page or test with curl as follows :</p> <p>Send a request to generate a new report using the sample documentation :</p> <pre><code>curl -X POST \"http://127.0.0.1:8000/reports\" \\\n     -H \"accept: application/json\" \\\n     -F \"github_repo_url=https://github.com/dockersamples/example-voting-app/\" \\\n     -F \"documentation_files=@plat-md.pdf\"\n</code></pre> <p>Make a note of the reports staut_endpoint in the output</p> <p>Once completed you can view the report with, make sure to replace status_endpoint placeholder :</p> <pre><code>curl http://127.0.0.1:8000&lt;status_endpoint&gt;\n</code></pre> <p>If you have it installed you can also pipe the output to <code>jq</code> for readability.</p> <pre><code>curl http://127.0.0.1:8000&lt;status_endpoint&gt; | jq\n</code></pre>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/#build-and-run-a-container","title":"Build and run a container","text":"<p>A sample Dockerfile is provided, modify to suit your needs You can build a container with :</p> <pre><code>docker build -t mesop-backend:latest .\n</code></pre> <p>You need to pass ENVARS to the container when it runs, you can use the .env file mentioned above :</p> <pre><code>docker run --env-file .env mesop-backend:latest\n</code></pre> <p>TODO: Give example to pass ADC into the container when running locally</p>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/plat-md/","title":"Plat md","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/plat-md/#internal-developer-platform-documentation","title":"Internal Developer Platform Documentation","text":"<p>1. Introduction</p> <p>Overview of the Internal Developer Platform (IDP)</p> <ul> <li>Purpose and Benefits: The IDP is designed to streamline and accelerate the software development process within the organization. It provides a centralized platform for developers to access tools, resources, and services, fostering collaboration and efficiency.  </li> <li>Key Components: </li> <li>Unified Toolset: The IDP integrates a wide range of development tools, including version control systems, build automation tools, and testing frameworks, providing developers with a consistent and efficient workflow.  </li> <li>Deployment Automation: Automated pipelines for building, testing, and deploying applications, reducing manual effort and minimizing the risk of errors.  </li> <li>Monitoring and Analytics: Tools for monitoring application performance and collecting usage data, providing insights for optimization and troubleshooting.  </li> <li>Self-Service Capabilities: The IDP empowers developers with self-service capabilities, allowing them to provision resources, manage environments, and deploy applications independently, without relying on manual intervention from operations teams.  </li> <li>Governance and Security: The IDP incorporates robust governance and security controls to ensure compliance with organizational policies and protect sensitive data.  </li> <li>Collaboration and Knowledge Sharing: The IDP fosters a culture of collaboration and knowledge sharing among developers through features such as wikis, forums, and chat channels.</li> </ul> <p>2. Onboarding Requirements</p> <ul> <li>Internal Billing ID: </li> <li>The internal billing ID is used to track and manage financial records for accurate billing and revenue reporting.  </li> <li>Here is a list of the appropriate internal billing ID\u2019s by department , please identify the one that corresponds to your application and use it during the onboarding process :</li> </ul> Department Name Billing ID Engineering DEPT-ENG-1001 Product Management DEPT-PRD-2053 Marketing DEPT-MKT-3117 Sales DEPT-SAL-4890 Human Resources DEPT-HRS-5024 Finance DEPT-FIN-6731 Information Technology DEPT-ITS-7448 Customer Support DEPT-SUP-8192 Research &amp; Development DEPT-RND-9005 Legal DEPT-LEG-1567 <p>Example/Test Billing ID:</p> <p>For testing purposes or onboarding the sample application provided with the platform documentation, you can use the following billing ID:</p> <ul> <li> <p>Test Billing ID: <code>DEPT-SAMPLE-12345</code> </p> </li> <li> <p>Application Registration (Internal CMDB):   Registering applications within the CMDB is essential for maintaining an accurate, up-to-date repository of configuration items (CIs) and their relationships. This centralized source of truth enables numerous critical IT management functions:  </p> </li> <li>Incident and Problem Management: Quickly identify impacted applications during outages, streamlining troubleshooting and root cause analysis.  </li> <li>Change Management: Assess potential ripple effects of changes, ensuring that modifications are implemented safely and with minimal disruption.  </li> <li>Asset Management: Track application lifecycles, from procurement to retirement, facilitating license compliance and cost optimization.  </li> <li>Capacity Planning: Forecast resource requirements based on application dependencies and usage patterns, preventing performance bottlenecks.  </li> <li>Service Level Management: Establish and monitor SLAs based on accurate application information, ensuring that business needs are met.  </li> <li>Compliance and Auditing: Demonstrate regulatory compliance by maintaining a comprehensive inventory of applications and their configurations.  </li> <li>Security Management: Identify and mitigate vulnerabilities by understanding application interdependencies and access controls.   By maintaining an accurate and comprehensive CMDB, organizations can improve operational efficiency, reduce downtime, enhance security, and ensure that IT services align with business objectives.</li> </ul> <p>Registering your application in the company's internal Configuration Management Database (CMDB) is a mandatory step before onboarding it to the Internal Developer Platform. This ensures your application is properly tracked for asset management, incident response, change management, and compliance purposes.  </p> <p>Steps to Register your App in the CMDB:</p> <ol> <li>Access the CMDB Portal: Navigate to the internal CMDB registration site, typically found at: <code>http://your-internal-cmdb.company.com</code> (Note: Replace with your actual internal URL). You may need to log in with your standard company credentials.  </li> <li>Initiate New Registration: Look for an option like \"Register New Application,\" \"New Configuration Item,\" or similar.  </li> <li>Complete Registration Form: Fill out the required information about your application. You will likely need to answer questions such as:  </li> <li>Application Name: The official, user-facing name of your application.  </li> <li>Brief Description: A short summary of the application's purpose and functionality.  </li> <li>Business Owner / Sponsoring Team: The primary business contact or team responsible for the application.  </li> <li>Technical Owner / Development Team: The primary technical contact or team responsible for development and maintenance.  </li> <li>Application Criticality: The level of impact to the business if the application were unavailable (e.g., High, Medium, Low).  </li> <li>Data Sensitivity: Classification of the type of data the application handles (e.g., Public, Internal, Confidential, Restricted).  </li> <li>Submit for Approval: Once the form is complete, submit it for registration. Depending on the process, this might involve an approval step.  </li> <li>Receive Application Identifier: Upon successful registration and approval, the CMDB system will generate a unique Application Identifier (sometimes called a CMDB ID or Asset Tag). This identifier is crucial for linking your application across various internal systems.  </li> <li>Record the Identifier: Carefully record this Application Identifier. You will need to provide it during the onboarding process for the Internal Developer Platform to associate your deployed resources correctly.</li> </ol> <p>Example/Test Identifier:</p> <p>For testing purposes or onboarding the sample application provided with the platform documentation, you can use the following identifier:</p> <ul> <li>Test Application Identifier: <code>APP-SAMPLE-12345</code></li> </ul> <p>Prerequisites:</p> <ul> <li>Required access (e.g., GitHub, Google Cloud).  </li> <li>Any necessary local setup (e.g., Git, Docker Desktop).</li> </ul> <p>3. Golden Paths: Architecture &amp; Usage</p> <ul> <li>The \"golden paths\" represent the officially supported, recommended, and optimized ways to build and deploy applications using this Internal Developer Platform (IDP). They are designed to provide a streamlined, efficient, and reliable experience by leveraging pre-configured tooling, automation (like the CI/CD pipelines triggered via GitHub), and infrastructure patterns managed by the platform team. Following a golden path ensures easier onboarding, better maintainability, standardized monitoring, and quicker access to support. While other approaches might be possible, deviating from a golden path may require significantly more effort from development teams to configure, manage, and maintain their application's infrastructure and deployment processes.   The platform utilizes GitHub as the central repository for all application source code. As part of the onboarding process for a new project or application, the platform team will provide you with a dedicated GitHub repository. This repository comes pre-configured with a Cloud Build trigger specifically linked to it. Once you commit your code changes to the <code>main</code> branch of this provided repository, the associated Cloud Build pipeline will automatically kick off, initiating the build, test, and deployment process defined within your chosen golden path.   Currently, there are two primary golden paths available:</li> </ul> <p>3.1. Golden Path 1: Containerized Applications on GKE Autopilot</p> <ul> <li>Overview: This path is the standard and recommended approach for most new development. It is ideal for applications that are already containerized or can be easily packaged into Open Container Initiative (OCI) containers. GKE Autopilot abstracts away much of the underlying infrastructure management, allowing teams to focus primarily on their application code.  </li> <li>Suitable Application Examples: </li> <li>Stateless web applications and APIs.  </li> <li>Microservices architectures where individual services are deployed as separate unprivileged containers.  </li> <li>Background processing workers or queue consumers.  </li> <li>Applications that benefit from automatic scaling based on load.  </li> <li>Architecture: <ul> <li>Need a diagram of the GCP Components </li> <li>Key Components: GitHub, Cloud Build, Artifact Registry, GKE Autopilot, Cloud SQL (Postgres), Secret Manager, Cloud Load Balancing, IAM.  </li> </ul> </li> <li> <p>Workflow: </p> <ul> <li>Steps:  </li> <li>Code commit to GitHub  </li> <li>Cloud Build trigger initiated  </li> <li>Cloud Build performs build and test routines  </li> <li>Container image pushed to Artifact Registry  </li> <li>Deployment triggered to GKE Autopilot  </li> <li>Cluster pulls image from Artifact Registry</li> </ul> <p></p> </li> <li> <p>Getting Started Guide: Need a Step-by-step tutorial for deploying a sample containerized application. </p> </li> <li> <p>3.2. Golden Path 2: Applications on Google Compute Engine (GCE) Instances</p> </li> <li> <p>Overview: This path provides more direct control over the operating system and underlying virtual machine environment. It's suitable for applications that have specific OS-level dependencies, require configurations not easily achievable within containers, or cannot be easily containerized. While offering flexibility, this path generally involves more infrastructure management responsibility for the development team compared to the GKE Autopilot path.  </p> </li> <li>Suitable Application Examples: </li> <li>Legacy, monolithic applications that are difficult or costly to containerize.  </li> <li>Applications requiring specific OS packages, kernel modules, or hardware configurations only available on GCE.  </li> <li>Certain types of stateful applications where managing state via external services like Cloud SQL is not feasible and instance-level state is preferred (use with caution).  </li> <li>Specialized third-party software requiring installation directly onto a virtual machine.  </li> <li>Architecture: <ul> <li>(Diagram/Image illustrating the flow needed here) </li> <li>Key Components: GitHub, Cloud Build, Cloud Storage (for artifacts), Compute Engine (GCE), Cloud SQL (Postgres), Secret Manager, Cloud Load Balancing  </li> </ul> </li> <li>Workflow: <ul> <li>Steps:  </li> <li>Code commit to GitHub  </li> <li>Cloud Build trigger initiated  </li> <li>Cloud Build performs build and test routines  </li> <li>Build artifacts stored in Cloud Storage  </li> <li>Deployment/Configuration process updates GCE instances </li> </ul> </li> </ul> <p>Updates to GCE instances within the Managed Instance Group (MIG) for this golden path are handled using GCE Startup Scripts. The Instance Template configured for the MIG includes a startup script designed to prepare the instance and run the application.   Here's how the update process works: 1. Artifact Storage: As part of the CI/CD process (Step 4), Cloud Build places the latest build artifacts (e.g., application binaries, configuration files) into a designated location within Cloud Storage.   2. Startup Script Execution: When a new GCE instance is created within the MIG (either initially or during an update/scaling event), the defined startup script automatically executes. 3. Fetching Artifacts: The startup script contains logic to securely download the latest application artifacts from the specific Cloud Storage location where Cloud Build placed them. 4. Configuration &amp; Startup: After fetching the artifacts, the script performs any necessary setup, such as installing dependencies, applying configurations, and finally, starting the application service. 5. Triggering Updates: To deploy a new version, an update is triggered on the Managed Instance Group (MIG) itself. This involves initiating a rolling replacement of the instances. As old instances are terminated and new ones are created based on the existing instance template, these new instances automatically run the startup script, pulling the latest artifacts and configurations you deployed to Cloud Storage.</p> <ul> <li>Getting Started Guide: Need a Step-by-step tutorial for deploying a sample application onto a GCE instance.</li> </ul> <p>4. Onboarding to the Platform</p> <p>TODO</p> <p>5. Platform Deep Dive</p> <p>5.1 Services Available </p> <ul> <li>GKE Autopilot: This service is the foundation of our platform, managing and orchestrating containerized applications. It automates many operational tasks, such as scaling, node provisioning, and maintenance, allowing developers to focus on application development.  </li> <li>Cloud Build: This service is our CI/CD pipeline, automating the building, testing, and deployment of our applications. It integrates with our source code repository and GKE Autopilot to enable fast and reliable software delivery.  </li> <li>Cloud Storage: This service provides scalable and durable object storage for our platform. It stores application data, backups, and other artifacts including code deployment artifacts.  </li> <li>Cloud SQL (Postgres): This service is our managed relational database. It stores structured application data and provides high availability, scalability, and security.  </li> <li>Artifact Registry: This service stores and manages container images and other build artifacts. It integrates with Cloud Build and GKE Autopilot to enable efficient and secure image management.  </li> <li>Compute Engine: This service provides virtual machines that can be used for various purposes, such as running batch jobs or hosting custom applications.  </li> <li>IAM: This service manages access control and permissions for our platform. It ensures that only authorized users and services can access our resources.  </li> <li>Cloud Load Balancing: This service distributes incoming traffic across multiple instances of our application, ensuring high availability and performance.  </li> <li>Secret Manager: This service stores and manages sensitive data, such as API keys and database passwords. It integrates with our other services to provide secure access to secrets.</li> </ul> <p>5.2 Platform Architecture</p> <p>This section provides a high-level overview of the Internal Developer Platform (IDP) architecture itself \u2013 the components that run the platform and orchestrate the developer experience.</p> <p></p> <p>The Internal Developer Platform utilizes a hybrid approach, primarily leveraging a GitOps workflow facilitated by Google Cloud's Config Sync and Config Controller running within the GKE Autopilot cluster, but retaining direct API interaction for external resources like GitHub.</p> <p>Here's how the provisioning process works with this model:</p> <ol> <li>Onboarding Request &amp; Initial GitHub Repo Creation: When a developer onboards an application via the IDP portal/API, the IDP Control Plane (Cloud Run service) first interacts directly with the GitHub API (using appropriate credentials/libraries) to create the dedicated source code repository for the application.  </li> <li>Manifest Generation: Once the repository is created, the IDP Control Plane generates a set of declarative configuration manifests. These manifests adhere to the Kubernetes Resource Model (KRM) and define:  </li> <li>Standard Kubernetes resources needed within the application's namespace (e.g., NetworkPolicies, ResourceQuotas).  </li> <li>Google Cloud resources required by the application, expressed as KRM objects (e.g., SQLInstance, ServiceAccount, IAMPolicyMember, StorageBucket, ArtifactRegistryRepository, CloudBuildTrigger). (Note: Cloud Build Triggers can often be defined via KRM).  </li> <li>Initial application deployment manifests (e.g., Kubernetes Deployment, Service) and a baseline README.md.  </li> <li>Commit to Git: The IDP Control Plane commits these generated manifests to the newly created GitHub repository (or potentially a central configuration repository monitored by the platform). This Git commit becomes the desired state declaration for the application's Google Cloud infrastructure, Kubernetes configuration, and base application setup.  </li> <li>Config Sync Detection: Config Sync, running in the GKE cluster and configured to watch the relevant repository path, automatically detects the new or changed manifests committed by the IDP.  </li> <li>Applying K8s Resources: Config Sync directly applies any standard Kubernetes resource manifests (like Namespaces, NetworkPolicies) to the GKE cluster.  </li> <li>Config Controller Reconciliation: Config Sync forwards the KRM manifests for Google Cloud resources (like SQLInstance, ServiceAccount, CloudBuildTrigger etc.) to Config Controller.  </li> <li>GCP Resource Provisioning via Config Controller: Config Controller acts as a Kubernetes operator. It reads the desired state from the KRM manifests and makes the necessary calls to the corresponding Google Cloud APIs (e.g., Cloud SQL API, IAM API, Cloud Build API) to create or modify the actual cloud resources, continuously working to reconcile the live state with the desired state defined in Git.</li> </ol> <p>In this model, the initial setup of external resources like the GitHub repository requires direct API calls from the IDP Control Plane. However, the subsequent provisioning and management of Google Cloud resources and Kubernetes configurations are handled declaratively via manifests in Git, reconciled by Config Sync and Config Controller.</p> <p>5.3 IDP Control Plane</p> <p>The core of the IDP operates as a control plane hosted on Google Cloud. This control plane is responsible for managing platform state, handling user interactions, and automating resource provisioning. The key components are:</p> <ul> <li>Cloud Run Service(s) (IDP API / UI): One or more serverless Cloud Run services provide the main interface to the IDP. This includes the web portal developers use for onboarding and self-service actions, as well as the backend APIs that orchestrate the platform's functions.  </li> <li>Datastore (Platform Metadata): Google Cloud Datastore is used to store essential platform metadata. This includes information about onboarded users, registered applications, provisioned resource details, and other configuration required for the IDP to operate. The Cloud Run services interact with Datastore to store and retrieve this state information.</li> </ul> <p>5.4 Developer Onboarding and Resource Provisioning</p> <p>Developers interact with the IDP primarily through the API or UI hosted on Cloud Run. The typical workflow involves:</p> <ol> <li>Onboarding Request: A developer initiates the onboarding process for a new application via the IDP portal or API, providing necessary details like the Application Identifier (obtained from CMDB registration ) and the desired Golden Path.    </li> <li> <p>Automated Provisioning: Upon successful onboarding validation, the IDP Control Plane automatically provisions the necessary infrastructure and configuration for the application based on the chosen golden path. Common resources provisioned include:</p> </li> <li> <p>GitHub Repo: Creating a dedicated repository in GitHub for the application's source code.    </p> </li> <li>Cloud Build Trigger: Configuring a trigger linked to the GitHub repo to automatically start the CI/CD pipeline on commits to the main branch.    </li> <li>Service Account: Creating a dedicated Google Cloud Service Account with appropriate permissions for the application to interact with other Google Cloud services.    </li> <li>Secrets: Provisioning necessary secrets (e.g., database credentials, API keys) securely within Google Secret Manager.    </li> <li> <p>Cloud SQL DB / Cloud Storage Bucket (Optional): Provisioning a Cloud SQL database instance or a Cloud Storage bucket if requested during onboarding or required by the golden path.    </p> </li> <li> <p>Specific resources are provisioned based on the chosen path:</p> </li> <li> <p>For Golden Path 1 (GKE Autopilot):  </p> <ul> <li>An Artifact Registry Repo is set up to store the application's container images.    </li> <li>A dedicated GKE Autopilot Namespace is allocated within the shared cluster for the application's workloads.    </li> </ul> </li> <li> <p>For Golden Path 2 (GCE Instances):  </p> <ul> <li>A GCE Instance Template is configured, including the necessary startup scripts.    </li> <li>A Managed Instance Group (MIG) is created based on the instance template to manage the application's VMs.    </li> <li>Cloud Storage is used by the Cloud Build pipeline specifically to store build artifacts (e.g., application binaries, deployment scripts) that will be fetched by the GCE instances during startup.    </li> </ul> </li> <li> <p>Code Scaffolding and Configuration: As part of the setup, the IDP may also:</p> </li> <li> <p>Generate Files: Create initial configuration files like baseline Kubernetes manifests (for Path 1) or deployment scripts.  </p> </li> <li>Generate Documentation: Create a basic <code>README.md</code> file outlining the provisioned infrastructure.  </li> <li>Push to Repo: Commit these generated files to the newly provisioned GitHub repository to provide a starting point for the developer.</li> </ol> <p>6. Support &amp; Contact</p> <p>If you encounter issues, have questions, or need assistance with the Internal Developer Platform (IDP), here are the primary ways to get support:</p> <ul> <li> <p>Slack Channel (for quick questions &amp; discussion):</p> </li> <li> <p>Join the <code>#idp-support</code> channel in Slack (Note: Replace with your actual chat channel details)  </p> </li> <li>This is the best place for general questions, quick troubleshooting help, sharing tips with other users, and staying updated on platform announcements. The platform team monitors this channel during business hours.  </li> <li> <p>Ticketing System (for bug reports &amp; feature requests):</p> </li> <li> <p>For reporting bugs, requesting new features, or tracking issues that require more in-depth investigation, please file a ticket in the company's standard ticketing system (e.g., Jira, ServiceNow).  </p> </li> <li>Be sure to assign the ticket to the <code>IDP Support</code> queue or component (Note: Replace with your actual team support queue/bug component)  </li> <li>Please include detailed information, such as steps to reproduce the issue, error messages, relevant application identifiers, and the golden path you are using.</li> </ul> <p>For direct inquiries to the platform team, especially for questions not suitable for the public Slack channel or formal ticketing (e.g., onboarding coordination, specific architectural discussions), you can reach out via email:</p> <ul> <li>Email: <code>idp-team@yourcompany.com</code> (Note: Replace with your actual team email address).</li> </ul>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/backend/prompt_template/","title":"Migration Readiness Analysis Request","text":"<p>Analyze the readiness of an application for migration to the specified target platform. Base your analysis on the provided GitHub repository link, the extracted repository content below, and any accompanying uploaded platform documents (like PDFs, images, etc., if supplied directly).</p> <p>Target Platform: {target_platform} GitHub Repository: {github_repo_url}</p> <p>Extracted Repository Information:</p> <p>README Content:</p> <p>Dependency Files Content (e.g., requirements.txt, package.json, pom.xml):</p> <p>Dockerfile Content:</p> <p>Analysis Task:</p> <p>Evaluate the application's migration readiness by synthesizing information from all provided sources: the repository details above (README, dependencies, Dockerfile), the general structure implied by the GitHub repository link, and the specifics of the target platform outlined in any accompanying uploaded documents.</p> <p>Consider potential challenges, risks, necessary steps, and technology factors. Pay attention to dependencies listed and how they might align or conflict with the target platform described in the uploaded documentation. Note any containerization details from the Dockerfile.</p> <p>Output Format Instructions:</p> <p>IMPORTANT: Format your entire response as a single, valid JSON object. Do not include any text outside of the JSON structure (like introductory sentences or markdown formatting). The JSON object should strictly follow the structure outlined below:</p> <p>```json {{   \"readiness_assessment\": {{     \"score_category\": \"\",     \"score_value\": , // Optional numerical score 1-10     \"summary\": \"\"   }},   \"key_challenges\": [     // Include challenges identified from dependencies, Dockerfile, README, AND potential conflicts with platform docs     {{       \"id\": \"\",       \"title\": \"\",       \"description\": \"\",       \"severity\": \"\"     }}     // ... include other challenge objects   ],   \"recommended_steps\": [     // Include steps relevant to code changes, configuration, and platform specifics     {{       \"id\": \"\",       \"title\": \"\",       \"description\": \"\",       \"priority\":  // Lower number means higher priority     }}     // ... include other step objects   ],   \"technology_notes\": [     // Infer technologies from dependencies, Dockerfile, README     \"\"     // ... include other relevant notes   ],   \"documentation_notes\": [      // Summarize key aspects of the target platform based only on the uploaded documents     \"\"     // ... include other relevant notes from platform docs   ] }}"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/","title":"Internal Developer Platform Documentation","text":"<p>1. Introduction</p>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/#overview-of-the-internal-developer-platform-idp","title":"Overview of the Internal Developer Platform (IDP)","text":"<ul> <li>Purpose and Benefits: The IDP is designed to streamline and accelerate the software development process within the organization. It provides a centralized platform for developers to access tools, resources, and services, fostering collaboration and efficiency.  </li> <li>Key Components: </li> <li>Unified Toolset: The IDP integrates a wide range of development tools, including version control systems, build automation tools, and testing frameworks, providing developers with a consistent and efficient workflow.  </li> <li>Deployment Automation: Automated pipelines for building, testing, and deploying applications, reducing manual effort and minimizing the risk of errors.  </li> <li>Monitoring and Analytics: Tools for monitoring application performance and collecting usage data, providing insights for optimization and troubleshooting.  </li> <li>Self-Service Capabilities: The IDP empowers developers with self-service capabilities, allowing them to provision resources, manage environments, and deploy applications independently, without relying on manual intervention from operations teams.  </li> <li>Governance and Security: The IDP incorporates robust governance and security controls to ensure compliance with organizational policies and protect sensitive data.  </li> <li>Collaboration and Knowledge Sharing: The IDP fosters a culture of collaboration and knowledge sharing among developers through features such as wikis, forums, and chat channels.</li> </ul> <p>2. Onboarding Requirements</p> <ul> <li>Internal Billing ID: </li> <li>The internal billing ID is used to track and manage financial records for accurate billing and revenue reporting.  </li> <li>Here is a list of the appropriate internal billing IDs by department , please identify the one that corresponds to your application and use it during the onboarding process :</li> </ul> Department Name Billing ID Engineering DEPT-ENG-1001 Product Management DEPT-PRD-2053 Marketing DEPT-MKT-3117 Sales DEPT-SAL-4890 Human Resources DEPT-HRS-5024 Finance DEPT-FIN-6731 Information Technology DEPT-ITS-7448 Customer Support DEPT-SUP-8192 Research &amp; Development DEPT-RND-9005 Legal DEPT-LEG-1567 <p>Example/Test Billing ID:</p> <p>For testing purposes or onboarding the sample application provided with the platform documentation, you can use the following billing ID:</p> <ul> <li> <p>Test Billing ID: <code>DEPT-SAMPLE-12345</code></p> </li> <li> <p>Application Registration (Internal CMDB):   Registering applications within the CMDB is essential for maintaining an accurate, up-to-date repository of configuration items (CIs) and their relationships. This centralized source of truth enables numerous critical IT management functions:  </p> </li> <li>Incident and Problem Management: Quickly identify impacted applications during outages, streamlining troubleshooting and root cause analysis.  </li> <li>Change Management: Assess potential ripple effects of changes, ensuring that modifications are implemented safely and with minimal disruption.  </li> <li>Asset Management: Track application lifecycles, from procurement to retirement, facilitating license compliance and cost optimization.  </li> <li>Capacity Planning: Forecast resource requirements based on application dependencies and usage patterns, preventing performance bottlenecks.  </li> <li>Service Level Management: Establish and monitor SLAs based on accurate application information, ensuring that business needs are met.  </li> <li>Compliance and Auditing: Demonstrate regulatory compliance by maintaining a comprehensive inventory of applications and their configurations.  </li> <li>Security Management: Identify and mitigate vulnerabilities by understanding application interdependencies and access controls.   By maintaining an accurate and comprehensive CMDB, organizations can improve operational efficiency, reduce downtime, enhance security, and ensure that IT services align with business objectives.</li> </ul> <p>Registering your application in the company's internal Configuration Management Database (CMDB) is a mandatory step before onboarding it to the Internal Developer Platform. This ensures your application is properly tracked for asset management, incident response, change management, and compliance purposes.  </p> <p>Steps to Register your App in the CMDB:</p> <ol> <li>Access the CMDB Portal: Navigate to the internal CMDB registration site, typically found at: <code>http://your-internal-cmdb.company.com</code> (Note: Replace with your actual internal URL). You may need to log in with your standard company credentials.  </li> <li>Initiate New Registration: Look for an option like \"Register New Application,\" \"New Configuration Item,\" or similar.  </li> <li>Complete Registration Form: Fill out the required information about your application. You will likely need to answer questions such as:  </li> <li>Application Name: The official, user-facing name of your application.  </li> <li>Brief Description: A short summary of the application's purpose and functionality.  </li> <li>Business Owner / Sponsoring Team: The primary business contact or team responsible for the application.  </li> <li>Technical Owner / Development Team: The primary technical contact or team responsible for development and maintenance.  </li> <li>Application Criticality: The level of impact to the business if the application were unavailable (e.g., High, Medium, Low).  </li> <li>Data Sensitivity: Classification of the type of data the application handles (e.g., Public, Internal, Confidential, Restricted).  </li> <li>Submit for Approval: Once the form is complete, submit it for registration. Depending on the process, this might involve an approval step.  </li> <li>Receive Application Identifier: Upon successful registration and approval, the CMDB system will generate a unique Application Identifier (sometimes called a CMDB ID or Asset Tag). This identifier is crucial for linking your application across various internal systems.  </li> <li>Record the Identifier: Carefully record this Application Identifier. You will need to provide it during the onboarding process for the Internal Developer Platform to associate your deployed resources correctly.</li> </ol> <p>Example/Test Identifier:</p> <p>For testing purposes or onboarding the sample application provided with the platform documentation, you can use the following identifier:</p> <ul> <li>Test Application Identifier: <code>APP-SAMPLE-12345</code></li> </ul> <p>Prerequisites:</p> <ul> <li>Required access (e.g., GitHub, Google Cloud).  </li> <li>Any necessary local setup (e.g., Git, Docker Desktop).</li> </ul> <p>3. Golden Paths: Architecture &amp; Usage</p> <ul> <li>The \"golden paths\" represent the officially supported, recommended, and optimized ways to build and deploy applications using this Internal Developer Platform (IDP). They are designed to provide a streamlined, efficient, and reliable experience by leveraging pre-configured tooling, automation (like the CI/CD pipelines triggered via GitHub), and infrastructure patterns managed by the platform team. Following a golden path ensures easier onboarding, better maintainability, standardized monitoring, and quicker access to support. While other approaches might be possible, deviating from a golden path may require significantly more effort from development teams to configure, manage, and maintain their application's infrastructure and deployment processes.   The platform utilizes GitHub as the central repository for all application source code. As part of the onboarding process for a new project or application, the platform team will provide you with a dedicated GitHub repository. This repository comes pre-configured with a Cloud Build trigger specifically linked to it. Once you commit your code changes to the <code>main</code> branch of this provided repository, the associated Cloud Build pipeline will automatically kick off, initiating the build, test, and deployment process defined within your chosen golden path.   Currently, there are two primary golden paths available:</li> </ul> <p>3.1. Golden Path 1: Containerized Applications on GKE Autopilot</p> <ul> <li>Overview: This path is the standard and recommended approach for most new development. It is ideal for applications that are already containerized or can be easily packaged into Open Container Initiative (OCI) containers. GKE Autopilot abstracts away much of the underlying infrastructure management, allowing teams to focus primarily on their application code.  </li> <li>Suitable Application Examples: </li> <li>Stateless web applications and APIs.  </li> <li>Microservices architectures where individual services are deployed as separate unprivileged containers.  </li> <li>Background processing workers or queue consumers.  </li> <li>Applications that benefit from automatic scaling based on load.  </li> <li>Architecture: <ul> <li>Need a diagram of the GCP Components </li> <li>Key Components: GitHub, Cloud Build, Artifact Registry, GKE Autopilot, Cloud SQL (Postgres), Secret Manager, Cloud Load Balancing, IAM.  </li> </ul> </li> <li> <p>Workflow: </p> <ul> <li>Steps:  </li> <li>Code commit to GitHub  </li> <li>Cloud Build trigger initiated  </li> <li>Cloud Build performs build and test routines  </li> <li>Container image pushed to Artifact Registry  </li> <li>Deployment triggered to GKE Autopilot  </li> <li>Cluster pulls image from Artifact Registry</li> </ul> <p></p> </li> <li> <p>Getting Started Guide: Need a Step-by-step tutorial for deploying a sample containerized application. </p> </li> <li> <p>3.2. Golden Path 2: Applications on Google Compute Engine (GCE) Instances</p> </li> <li> <p>Overview: This path provides more direct control over the operating system and underlying virtual machine environment. It's suitable for applications that have specific OS-level dependencies, require configurations not easily achievable within containers, or cannot be easily containerized. While offering flexibility, this path generally involves more infrastructure management responsibility for the development team compared to the GKE Autopilot path.  </p> </li> <li>Suitable Application Examples: </li> <li>Legacy, monolithic applications that are difficult or costly to containerize.  </li> <li>Applications requiring specific OS packages, kernel modules, or hardware configurations only available on GCE.  </li> <li>Certain types of stateful applications where managing state via external services like Cloud SQL is not feasible and instance-level state is preferred (use with caution).  </li> <li>Specialized third-party software requiring installation directly onto a virtual machine.  </li> <li>Architecture: <ul> <li>(Diagram/Image illustrating the flow needed here) </li> <li>Key Components: GitHub, Cloud Build, Cloud Storage (for artifacts), Compute Engine (GCE), Cloud SQL (Postgres), Secret Manager, Cloud Load Balancing  </li> </ul> </li> <li>Workflow: <ul> <li>Steps:  </li> <li>Code commit to GitHub  </li> <li>Cloud Build trigger initiated  </li> <li>Cloud Build performs build and test routines  </li> <li>Build artifacts stored in Cloud Storage  </li> <li>Deployment/Configuration process updates GCE instances </li> </ul> </li> </ul> <p>Updates to GCE instances within the Managed Instance Group (MIG) for this golden path are handled using GCE Startup Scripts. The Instance Template configured for the MIG includes a startup script designed to prepare the instance and run the application.   Here's how the update process works:</p> <ol> <li>Artifact Storage: As part of the CI/CD process (Step 4), Cloud Build places the latest build artifacts (e.g., application binaries, configuration files) into a designated location within Cloud Storage.</li> <li>Startup Script Execution: When a new GCE instance is created within the MIG (either initially or during an update/scaling event), the defined startup script automatically executes.  </li> <li>Fetching Artifacts: The startup script contains logic to securely download the latest application artifacts from the specific Cloud Storage location where Cloud Build placed them.  </li> <li>Configuration &amp; Startup: After fetching the artifacts, the script performs any necessary setup, such as installing dependencies, applying configurations, and finally, starting the application service.  </li> <li> <p>Triggering Updates: To deploy a new version, an update is triggered on the Managed Instance Group (MIG) itself. This involves initiating a rolling replacement of the instances. As old instances are terminated and new ones are created based on the existing instance template, these new instances automatically run the startup script, pulling the latest artifacts and configurations you deployed to Cloud Storage.</p> </li> <li> <p>Getting Started Guide: Need a Step-by-step tutorial for deploying a sample application onto a GCE instance.</p> </li> </ol> <p>4. Onboarding to the Platform</p> <p>TODO</p>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/#5-platform-deep-dive","title":"5. Platform Deep Dive","text":""},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/#51-services-available","title":"5.1 Services Available","text":"<ul> <li>GKE Autopilot: This service is the foundation of our platform, managing and orchestrating containerized applications. It automates many operational tasks, such as scaling, node provisioning, and maintenance, allowing developers to focus on application development.  </li> <li>Cloud Build: This service is our CI/CD pipeline, automating the building, testing, and deployment of our applications. It integrates with our source code repository and GKE Autopilot to enable fast and reliable software delivery.  </li> <li>Cloud Storage: This service provides scalable and durable object storage for our platform. It stores application data, backups, and other artifacts including code deployment artifacts.  </li> <li>Cloud SQL (Postgres): This service is our managed relational database. It stores structured application data and provides high availability, scalability, and security.  </li> <li>Artifact Registry: This service stores and manages container images and other build artifacts. It integrates with Cloud Build and GKE Autopilot to enable efficient and secure image management.  </li> <li>Compute Engine: This service provides virtual machines that can be used for various purposes, such as running batch jobs or hosting custom applications.  </li> <li>IAM: This service manages access control and permissions for our platform. It ensures that only authorized users and services can access our resources.  </li> <li>Cloud Load Balancing: This service distributes incoming traffic across multiple instances of our application, ensuring high availability and performance.  </li> <li>Secret Manager: This service stores and manages sensitive data, such as API keys and database passwords. It integrates with our other services to provide secure access to secrets.</li> </ul>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/#52-platform-architecture","title":"5.2 Platform Architecture","text":"<p>This section provides a high-level overview of the Internal Developer Platform (IDP) architecture itself \u2013 the components that run the platform and orchestrate the developer experience.</p> <p></p> <p>The Internal Developer Platform utilizes a hybrid approach, primarily leveraging a GitOps workflow facilitated by Google Cloud's Config Sync and Config Controller running within the GKE Autopilot cluster, but retaining direct API interaction for external resources like GitHub.</p> <p>Here's how the provisioning process works with this model:</p> <ol> <li>Onboarding Request &amp; Initial GitHub Repo Creation: When a developer onboards an application via the IDP portal/API, the IDP Control Plane (Cloud Run service) first interacts directly with the GitHub API (using appropriate credentials/libraries) to create the dedicated source code repository for the application.  </li> <li>Manifest Generation: Once the repository is created, the IDP Control Plane generates a set of declarative configuration manifests. These manifests adhere to the Kubernetes Resource Model (KRM) and define:  </li> <li>Standard Kubernetes resources needed within the application's namespace (e.g., NetworkPolicies, ResourceQuotas).  </li> <li>Google Cloud resources required by the application, expressed as KRM objects (e.g., SQLInstance, ServiceAccount, IAMPolicyMember, StorageBucket, ArtifactRegistryRepository, CloudBuildTrigger). (Note: Cloud Build Triggers can often be defined via KRM).  </li> <li>Initial application deployment manifests (e.g., Kubernetes Deployment, Service) and a baseline README.md.  </li> <li>Commit to Git: The IDP Control Plane commits these generated manifests to the newly created GitHub repository (or potentially a central configuration repository monitored by the platform). This Git commit becomes the desired state declaration for the application's Google Cloud infrastructure, Kubernetes configuration, and base application setup.  </li> <li>Config Sync Detection: Config Sync, running in the GKE cluster and configured to watch the relevant repository path, automatically detects the new or changed manifests committed by the IDP.  </li> <li>Applying K8s Resources: Config Sync directly applies any standard Kubernetes resource manifests (like Namespaces, NetworkPolicies) to the GKE cluster.  </li> <li>Config Controller Reconciliation: Config Sync forwards the KRM manifests for Google Cloud resources (like SQLInstance, ServiceAccount, CloudBuildTrigger etc.) to Config Controller.  </li> <li>GCP Resource Provisioning via Config Controller: Config Controller acts as a Kubernetes operator. It reads the desired state from the KRM manifests and makes the necessary calls to the corresponding Google Cloud APIs (e.g., Cloud SQL API, IAM API, Cloud Build API) to create or modify the actual cloud resources, continuously working to reconcile the live state with the desired state defined in Git.</li> </ol> <p>In this model, the initial setup of external resources like the GitHub repository requires direct API calls from the IDP Control Plane. However, the subsequent provisioning and management of Google Cloud resources and Kubernetes configurations are handled declaratively via manifests in Git, reconciled by Config Sync and Config Controller.</p>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/#53-idp-control-plane","title":"5.3 IDP Control Plane","text":"<p>The core of the IDP operates as a control plane hosted on Google Cloud. This control plane is responsible for managing platform state, handling user interactions, and automating resource provisioning. The key components are:</p> <ul> <li>Cloud Run Service(s) (IDP API / UI): One or more serverless Cloud Run services provide the main interface to the IDP. This includes the web portal developers use for onboarding and self-service actions, as well as the backend APIs that orchestrate the platform's functions.  </li> <li>Datastore (Platform Metadata): Google Cloud Datastore is used to store essential platform metadata. This includes information about onboarded users, registered applications, provisioned resource details, and other configuration required for the IDP to operate. The Cloud Run services interact with Datastore to store and retrieve this state information.</li> </ul>"},{"location":"reference-architectures/gemini-powered-migration-blocker-analysis/sample-platform/docs/platform-doc/#54-developer-onboarding-and-resource-provisioning","title":"5.4 Developer Onboarding and Resource Provisioning","text":"<p>Developers interact with the IDP primarily through the API or UI hosted on Cloud Run. The typical workflow involves:</p> <ol> <li>Onboarding Request: A developer initiates the onboarding process for a new application via the IDP portal or API, providing necessary details like the Application Identifier (obtained from CMDB registration ) and the desired Golden Path.</li> <li> <p>Automated Provisioning: Upon successful onboarding validation, the IDP Control Plane automatically provisions the necessary infrastructure and configuration for the application based on the chosen golden path. Common resources provisioned include:</p> </li> <li> <p>GitHub Repo: Creating a dedicated repository in GitHub for the application's source code.</p> </li> <li>Cloud Build Trigger: Configuring a trigger linked to the GitHub repo to automatically start the CI/CD pipeline on commits to the main branch.</li> <li>Service Account: Creating a dedicated Google Cloud Service Account with appropriate permissions for the application to interact with other Google Cloud services.</li> <li>Secrets: Provisioning necessary secrets (e.g., database credentials, API keys) securely within Google Secret Manager.</li> <li> <p>Cloud SQL DB / Cloud Storage Bucket (Optional): Provisioning a Cloud SQL database instance or a Cloud Storage bucket if requested during onboarding or required by the golden path.</p> </li> <li> <p>Specific resources are provisioned based on the chosen path:</p> </li> <li> <p>For Golden Path 1 (GKE Autopilot):  </p> <ul> <li>An Artifact Registry Repo is set up to store the application's container images.</li> <li>A dedicated GKE Autopilot Namespace is allocated within the shared cluster for the application's workloads.</li> </ul> </li> <li> <p>For Golden Path 2 (GCE Instances):  </p> <ul> <li>A GCE Instance Template is configured, including the necessary startup scripts.</li> <li>A Managed Instance Group (MIG) is created based on the instance template to manage the application's VMs.</li> <li>Cloud Storage is used by the Cloud Build pipeline specifically to store build artifacts (e.g., application binaries, deployment scripts) that will be fetched by the GCE instances during startup.</li> </ul> </li> <li> <p>Code Scaffolding and Configuration: As part of the setup, the IDP may also:</p> </li> <li> <p>Generate Files: Create initial configuration files like baseline Kubernetes manifests (for Path 1) or deployment scripts.  </p> </li> <li>Generate Documentation: Create a basic <code>README.md</code> file outlining the provisioned infrastructure.  </li> <li>Push to Repo: Commit these generated files to the newly provisioned GitHub repository to provide a starting point for the developer.</li> </ol> <p>6. Support &amp; Contact</p> <p>If you encounter issues, have questions, or need assistance with the Internal Developer Platform (IDP), here are the primary ways to get support:</p> <ul> <li> <p>Slack Channel (for quick questions &amp; discussion):</p> </li> <li> <p>Join the <code>#idp-support</code> channel in Slack (Note: Replace with your actual chat channel details)  </p> </li> <li>This is the best place for general questions, quick troubleshooting help, sharing tips with other users, and staying updated on platform announcements. The platform team monitors this channel during business hours.  </li> <li> <p>Ticketing System (for bug reports &amp; feature requests):</p> </li> <li> <p>For reporting bugs, requesting new features, or tracking issues that require more in-depth investigation, please file a ticket in the company's standard ticketing system (e.g., Jira, ServiceNow).  </p> </li> <li>Be sure to assign the ticket to the <code>IDP Support</code> queue or component (Note: Replace with your actual team support queue/bug component)  </li> <li>Please include detailed information, such as steps to reproduce the issue, error messages, relevant application identifiers, and the golden path you are using.</li> </ul> <p>For direct inquiries to the platform team, especially for questions not suitable for the public Slack channel or formal ticketing (e.g., onboarding coordination, specific architectural discussions), you can reach out via email:</p> <ul> <li>Email: <code>idp-team@yourcompany.com</code> (Note: Replace with your actual team email address).</li> </ul>"},{"location":"reference-architectures/github-runners-gke/","title":"Reference Guide: Deploy and use GitHub Actions Runners on GKE","text":""},{"location":"reference-architectures/github-runners-gke/#overview","title":"Overview","text":"<p>This guide walks you through the process of setting up self-hosted GitHub Actions Runners on Google Kubernetes Engine (GKE) using the Terraform module <code>terraform-google-github-actions-runners</code>. It then provides instructions on how to create a basic GitHub Actions workflow to leverage these runners.</p> <p></p>"},{"location":"reference-architectures/github-runners-gke/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terraform: Install Terraform on your local machine or use Cloud Shell</li> <li>Google Cloud Project: Have a Google Cloud project with a Billing Account   linked and the following APIs enabled:<ul> <li>Cloud Resource Manager API <code>cloudresourcemanager.googleapis.com</code></li> <li>Identity and Access Management API <code>iam.googleapis.com</code></li> <li>Kubernetes Engine API <code>container.googleapis.com</code></li> <li>Service Usage API <code>serviceusage.googleapis.com</code></li> </ul> </li> <li>GitHub Account: Have a GitHub organization, either personal or enterprise,   where you have administrator access.</li> </ul> <p>Run the following command to enable the prerequisite APIs:</p> <pre><code>gcloud services enable \\\n  cloudresourcemanager.googleapis.com \\\n  iam.googleapis.com \\\n  container.googleapis.com \\\n  serviceusage.googleapis.com \\\n  --project &lt;YOUR_PROJECT_ID&gt;\n</code></pre>"},{"location":"reference-architectures/github-runners-gke/#register-a-github-app-for-authenticating-arc","title":"Register a GitHub App for Authenticating ARC","text":"<p>Using a GitHub App for authentication allows you to make your self-hosted runners available to a GitHub organization that you own or have administrative access to. For more details on registering GitHub Apps, see GitHub\u2019s documentation.</p> <p>You will need 3 values from this section to use as inputs in the Terraform module:</p> <ul> <li>GitHub App ID</li> <li>GitHub App Private Key</li> <li>GitHub App Installation ID</li> </ul>"},{"location":"reference-architectures/github-runners-gke/#navigate-to-your-organization-github-app-settings","title":"Navigate to your Organization GitHub App settings","text":"<ol> <li>Click your profile picture in the top-right</li> <li>Click Your organizations</li> <li>Select the organization you want to use for this walkthrough</li> <li>Click Settings</li> <li>Click \\&lt;&gt; Developer settings</li> <li>Click GitHub Apps</li> </ol>"},{"location":"reference-architectures/github-runners-gke/#create-a-new-github-app","title":"Create a new GitHub App","text":"<ol> <li>Click New GitHub App</li> <li>Under \u201cGitHub App name\u201d, choose a unique name such as \u201cmy-gke-arc-app\u201d</li> <li>Under \u201cHomepage URL\u201d enter     <code>https://github.com/actions/actions-runner-controller</code></li> <li>Under \u201cWebhook,\u201d uncheck Active.</li> <li>Under \u201cPermissions,\u201d click Repository permissions and use the dropdown     menu to select the following permissions:<ol> <li>Metadata: Read-only</li> </ol> </li> <li>Under \u201cPermissions,\u201d click Organization permissions and use the dropdown     menu to select the following permissions:<ol> <li>Self-hosted runners: Read and write</li> </ol> </li> <li>Click the Create GitHub App button</li> </ol>"},{"location":"reference-architectures/github-runners-gke/#gather-required-ids-and-keys","title":"Gather required IDs and keys","text":"<ol> <li>On the GitHub App\u2019s page, save the value for \u201cApp ID\u201d<ol> <li>You will use this as the value for <code>gh_app_id</code> in the Terraform module</li> </ol> </li> <li>Under \u201cPrivate keys\u201d click Generate a private key. Save the <code>.pem</code> file     for later.<ol> <li>You will use this as the value for <code>gh_app_private_key</code> in the Terraform     module</li> </ol> </li> <li>In the menu at the top-left corner of the page, click Install App, and     next to your organization, click Install to install the app on your     organization.<ol> <li>Choose All repositories to allow any repository in your org to have     access to your runners</li> <li>Choose Only select repositories to allow specific repos to have     access to your runners</li> </ol> </li> <li>Note the app installation ID, which you can find on the app installation     page, which has the following URL format:     <code>https://github.com/organizations/ORGANIZATION/settings/installations/INSTALLATION_ID</code><ol> <li>You will use this as the value for <code>gh_app_installation_id</code> in the     Terraform module.</li> </ol> </li> </ol>"},{"location":"reference-architectures/github-runners-gke/#configure-terraform-example","title":"Configure Terraform example","text":""},{"location":"reference-architectures/github-runners-gke/#open-the-terraform-example","title":"Open the Terraform example","text":"<p>Open the Terraform module repository in Cloud Shell automatically by clicking the button:</p> <p></p> <p>Clicking this button will clone the repository into Cloud Shell, change into the example directory, and open the <code>main.tf</code> file in the Cloud Shell Editor.</p>"},{"location":"reference-architectures/github-runners-gke/#modify-terraform-example-variables","title":"Modify Terraform example variables","text":"<ol> <li>Insert your Google Cloud Project ID as the value of <code>project_id</code></li> <li>Modify the sample values of the following variables with the values you     saved from earlier.<ol> <li><code>gh_app_id</code>: insert the value of the App ID from the GitHub App page</li> <li><code>gh_app_installation_id</code>: insert the value from the URL of the app     installation page</li> <li><code>gh_app_private_key</code>:<ol> <li>Copy the <code>.pem</code> file to example directory, alongside the <code>main.tf</code>     file</li> <li>Insert the <code>.pem</code> filename you downloaded after generating the     private key for the app, like so:<ol> <li><code>gh_app_private_key = file(\"example.private-key.pem\")</code></li> </ol> </li> <li>Warning: Terraform will store the private key in state as plaintext.     It\u2019s recommended to secure your state file by using a backend such     as a GCS bucket with encryption. You can do so by following     these instructions.</li> </ol> </li> </ol> </li> <li>Modify the value of <code>gh_config_url</code> with the URL of your GitHub     organization. It will be in the format of <code>https://github.com/ORGANIZATION</code></li> <li>(Optional) Specify any other parameters that you wish. For a full list of     variables you can modify, refer to the     module documentation.</li> </ol>"},{"location":"reference-architectures/github-runners-gke/#deploy-the-example","title":"Deploy the example","text":"<ol> <li>Initialize Terraform: Run <code>terraform init</code> to download the required     providers.</li> <li>Plan: Run <code>terraform plan</code> to preview the changes that will be made.</li> <li>Apply: Run <code>terraform apply</code> and confirm to create the resources.</li> </ol> <p>You will see the runners become available in your GitHub Organization:</p> <ol> <li>Go to your GitHub organization page</li> <li>Click Settings</li> <li>Open the \u201cActions\u201d drop-down in the left menu and choose Runners</li> </ol> <p>You should see the runners appear as \u201carc-runners\u201d</p>"},{"location":"reference-architectures/github-runners-gke/#creating-a-github-actions-workflow","title":"Creating a GitHub Actions Workflow","text":"<ol> <li>Create a new GitHub repository within your organization.</li> <li>In your GitHub repository, click the Actions tab.</li> <li>Click New workflow</li> <li>Under \u201cChoose workflow\u201d click set up a workflow yourself</li> <li> <p>Paste the following configuration into the text editor:</p> <pre><code>name: Actions Runner Controller Demo\non:\nworkflow_dispatch:\njobs:\nExplore-GitHub-Actions:\n   runs-on: arc-runners\n   steps:\n   - run: echo \"This job uses runner scale set runners!\"\n</code></pre> </li> <li> <p>Click Commit changes to save the workflow to your repository.</p> </li> </ol>"},{"location":"reference-architectures/github-runners-gke/#test-the-github-actions-workflow","title":"Test the GitHub Actions Workflow","text":"<ol> <li>Go back to the Actions tab in your repository.</li> <li>In the left menu, select the name of your workflow. This should be \u201cActions     Runner Controller Demo\u201d if you left the above configuration unchanged</li> <li>Click Run workflow to open the drop-down menu, and click Run     workflow</li> <li>The sample workflow executes on your GKE-hosted ARC runner set. You can view     the output within the GitHub Actions run history.</li> </ol>"},{"location":"reference-architectures/github-runners-gke/#cleanup","title":"Cleanup","text":""},{"location":"reference-architectures/github-runners-gke/#teardown-terraform-managed-infrastructure","title":"Teardown Terraform-managed infrastructure","text":"<ol> <li> <p>Navigate back into the example directory you previously ran     <code>terraform apply</code></p> <pre><code>cd terraform-google-github-actions-runners/examples/gh-runner-gke-simple/\n</code></pre> </li> <li> <p>Destroy Terraform-managed infrastructure</p> <pre><code>terraform destroy\n</code></pre> </li> </ol> <p>Warning: this will destroy the GKE cluster, example VPC, service accounts, and the Helm-managed workloads previously deployed by this example.</p>"},{"location":"reference-architectures/github-runners-gke/#delete-github-resources","title":"Delete GitHub resources","text":"<p>If you created a new GitHub App for testing purposes of this walkthrough, you can delete it via the following instructions. Note that any services authenticating via this GitHub App will lose access.</p> <ol> <li>Navigate to your Organization GitHub App settings<ol> <li>Click your profile picture in the top-right</li> <li>Click Your organizations</li> <li>Select the organization you used for this walkthrough</li> <li>Click Settings</li> <li>Click the \\&lt;&gt; Developer settings drop-down</li> <li>Click GitHub Apps</li> </ol> </li> <li>In the row where your GitHub App is listed, click Edit</li> <li>In the left-side menu, click Advanced</li> <li>Click Delete GitHub App</li> <li>Type the name of the GitHub App to confirm and delete.</li> </ol>"}]}