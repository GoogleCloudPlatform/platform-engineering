# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig, SafetySetting, HarmCategory, Part
from google.api_core import exceptions as google_exceptions
import asyncio
from typing import List, Optional

# TODO: refactor with the new library : https://github.com/googleapis/python-genai

try:
    from gcp_utils import get_gcs_blob_metadata
except ImportError:
    print("Warning: Could not import get_gcs_blob_metadata from gcp_utils.")
    async def get_gcs_blob_metadata(blob_name: str, bucket_name: str): return None


# Flag to track initialization
_vertexai_initialized = False


# --- Define supported MIME types for Part.from_uri (adjust based on model docs and what we allow users to upload) ---
SUPPORTED_URI_MIME_TYPES = {
    "application/pdf",
    "image/jpeg",
    "image/png",
    "image/gif",
    "text/plain",
}

async def analyze_with_gemini(
    text_prompt: str,
    gcs_uris: List[str],
    bucket_name: str,
    project_id: str,
    location: str,
    model_name: str
) -> str:
    """
    Sends a multimodal prompt (text + supported GCS files via URI) to Gemini
    and returns the text response.

    Args:
        text_prompt: The main text part of the prompt.
        gcs_uris: A list of GCS URIs (gs://...) for potential file inclusion.
        bucket_name: The GCS bucket where the files reside.
        project_id: Your Google Cloud project ID.
        location: The GCP region where Vertex AI is configured (e.g., "us-central1").
        model_name: The name of the Gemini model to use (must support multimodal URI).

    Returns:
        The text response generated by the model, or an error message if failed.
    """
    global _vertexai_initialized
    print(f"Initializing Vertex AI for project '{project_id}' in location '{location}'...")
    try:
        # Simple check to avoid re-init if already done in this run
        if not _vertexai_initialized:
            vertexai.init(project=project_id, location=location)
            _vertexai_initialized = True
            print("Vertex AI initialized.")
        else:
            print("Vertex AI already initialized.")
    except Exception as e:
        print(f"Error initializing Vertex AI: {e}")
        # Reset flag if init failed? Maybe not necessary if exception prevents further execution.
        return f"Error: Vertex AI initialization failed: {e}"

    print(f"Loading Gemini model: {model_name}")
    try:
        model = GenerativeModel(model_name)

        # TODO: Tweak these, this setup was pulled from example docs
        
        # Configure generation parameters
        generation_config = GenerationConfig(
            temperature=0.7,
            max_output_tokens=8192, # Increased for potentially larger analysis from files
            top_p=0.95,
        )
        
        # TODO: Tweak these, this setup was pulled from example docs
        
        # Configure safety settings (example)
        safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }

        # --- Construct the multimodal 'contents' list ---
        contents = []

        # TODO: move the part of the prompt that specifies the JSON output into
        # a schema config instead

    
        # Add the main text prompt part first
        contents.append(Part.from_text(text_prompt))
        print(f"Added text prompt part")

        # Process GCS URIs to add supported files as Parts
        if gcs_uris:
            print(f"Processing {len(gcs_uris)} GCS URIs for multimodal input...")
            processed_files_count = 0
            # Fetch metadata sequentially for simplicity
            # asyncio.gather for parallel fetching if performance becomes an issue
            for gcs_uri in gcs_uris:
                if not gcs_uri or not gcs_uri.startswith(f"gs://{bucket_name}/"):
                    print(f"Warning: Skipping invalid or non-matching URI '{gcs_uri}' for bucket '{bucket_name}'.")
                    continue

                # Extract blob name from URI (handle potential empty blob name)
                blob_name = gcs_uri[len(f"gs://{bucket_name}/"):]
                if not blob_name:
                     print(f"Warning: Skipping URI '{gcs_uri}' - empty blob name.")
                     continue

                # Fetch metadata (returns tuple (uri, mime_type) or None)
                metadata_result = await get_gcs_blob_metadata(blob_name, bucket_name)

                if metadata_result:
                    fetched_uri, mime_type = metadata_result
                    if mime_type and mime_type in SUPPORTED_URI_MIME_TYPES:
                        try:
                            # Create Part from URI for supported types
                            file_part = Part.from_uri(fetched_uri, mime_type=mime_type)
                            contents.append(file_part)
                            processed_files_count += 1
                            print(f"Added Part for {fetched_uri} (MIME: {mime_type})")
                        except ValueError as ve:
                            print(f"ValueError creating Part for {fetched_uri} (MIME: {mime_type}): {ve}. Check if URI/MIME format is correct.")
                        except Exception as part_e:
                             print(f"Unexpected error creating Part for {fetched_uri}: {part_e}")
                    else:
                        print(f"Skipping {fetched_uri} - MIME type '{mime_type}' not in supported list ({SUPPORTED_URI_MIME_TYPES}) for direct URI input.")
                else:
                     print(f"Skipping {gcs_uri} - Failed to fetch metadata or blob not found.")
            print(f"Finished processing URIs. Added {processed_files_count} file Part(s).")
        else:
            print("No GCS URIs provided to process for multimodal input.")

        # --- Call Gemini with the combined contents ---
        if not contents:
             print("Error: No content parts generated (text prompt might be empty?). Cannot call Gemini.")
             return "Error: No content to send to Gemini."

        print(f"Sending {len(contents)} part(s) to Gemini model '{model_name}'...")
        response = await model.generate_content_async(
            contents, # Pass the list of Parts (text, files)
            generation_config=generation_config,
            safety_settings=safety_settings,
            # stream=False # Default is False, ensures full response
        )
        print("Received response from Gemini.")

        # --- Process the response ---
        if not response.candidates:
             print("Warning: No candidates returned from Gemini. Check prompt or safety settings.")
             try:
                 # Attempt to get feedback if available
                 feedback = response.prompt_feedback
                 block_reason = feedback.block_reason.name if hasattr(feedback, 'block_reason') and feedback.block_reason else "Unknown"
                 safety_ratings_str = "N/A"
                 if hasattr(feedback, 'safety_ratings'):
                      safety_ratings_str = ", ".join([f"{r.category.name}: {r.probability.name}" for r in feedback.safety_ratings])
                 return f"Error: Gemini response blocked or empty. Reason: {block_reason}. Ratings: [{safety_ratings_str}]"
             except Exception as feedback_e:
                 print(f"Error retrieving feedback details: {feedback_e}")
                 return "Error: Gemini response blocked or empty."

        # Extract text from the first candidate (standard way)
        try:
             # Access text via response.text helper property if available and simple
             generated_text = response.text
             print("Successfully extracted text from Gemini response using response.text.")
             return generated_text
        except ValueError as e:
            # Fallback if .text property fails (e.g., complex parts, function calls)
             print(f"Could not extract text using response.text ({e}). Trying parts extraction...")
             try:
                 if response.candidates[0].content.parts:
                      # Assuming the primary text response is the first part
                      generated_text = response.candidates[0].content.parts[0].text
                      print("Successfully extracted text from Gemini response parts.")
                      return generated_text
                 else:
                      print("Warning: Gemini response candidate has no parts.")
                      return "Error: Received empty response content from Gemini."
             except Exception as part_e:
                  print(f"Error extracting text from response parts: {part_e}")
                  return f"Error: Failed to extract text from response: {part_e}"

    except google_exceptions.PermissionDenied as e:
        print(f"Permission denied calling Vertex AI. Check service account roles: {e}")
        return f"Error: Permission denied calling Vertex AI: {e}"
    except google_exceptions.ResourceExhausted as e:
         print(f"Vertex AI quota exceeded: {e}")
         return f"Error: Vertex AI quota exceeded: {e}"
    except Exception as e:
        print(f"An unexpected error occurred calling Gemini: {e}")
        import traceback
        traceback.print_exc() # Print full traceback for unexpected errors
        return f"Error: An unexpected error occurred: {e}"